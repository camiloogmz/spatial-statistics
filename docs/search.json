[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "spatial-statistics",
    "section": "",
    "text": "Welcome!\nOn this website, you will find the projects I developed for the Spatial Statistics course at the National University of Colombia. These projects mainly focus on the geostatistical analysis of weather conditions in Texas (USA) using several approaches, as well as the analysis of areal data from Nepal. All analyses were conducted using R."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "geostatistics/temp.html",
    "href": "geostatistics/temp.html",
    "title": "Análisis de la Temperatura",
    "section": "",
    "text": "Estudiar e identificar la tendencia y los patrones espaciales del material particulado fino (PM2.5) y de la temperatura mediante el uso de herramientas de análisis geoestadístico, con el propósito de establecer modelos de predicción espacial que caractericen su comportamiento en el estado de Texas durante el año 2024.\n\n\n\n\nAnalizar la estacionariedad en media de cada variable, estudiando la estructura de la media e identificando la existencia de dependencia espacial.\n\nConstruir el semivariograma empírico para PM2.5 y temperatura, y ajustar un modelo teórico que describa adecuadamente la estructura de variabilidad espacial.\n\nAplicar la técnica de Kriging para generar mapas de predicción espacial y mapas de incertidumbre, representando la distribución esperada de ambas variables y evaluando la precisión de las estimaciones obtenidas."
  },
  {
    "objectID": "geostatistics/temp.html#objetivo-general",
    "href": "geostatistics/temp.html#objetivo-general",
    "title": "Análisis de la Temperatura",
    "section": "",
    "text": "Estudiar e identificar la tendencia y los patrones espaciales del material particulado fino (PM2.5) y de la temperatura mediante el uso de herramientas de análisis geoestadístico, con el propósito de establecer modelos de predicción espacial que caractericen su comportamiento en el estado de Texas durante el año 2024."
  },
  {
    "objectID": "geostatistics/temp.html#objetivos-específicos",
    "href": "geostatistics/temp.html#objetivos-específicos",
    "title": "Análisis de la Temperatura",
    "section": "",
    "text": "Analizar la estacionariedad en media de cada variable, estudiando la estructura de la media e identificando la existencia de dependencia espacial.\n\nConstruir el semivariograma empírico para PM2.5 y temperatura, y ajustar un modelo teórico que describa adecuadamente la estructura de variabilidad espacial.\n\nAplicar la técnica de Kriging para generar mapas de predicción espacial y mapas de incertidumbre, representando la distribución esperada de ambas variables y evaluando la precisión de las estimaciones obtenidas."
  },
  {
    "objectID": "geostatistics/temp.html#descripción",
    "href": "geostatistics/temp.html#descripción",
    "title": "Análisis de la Temperatura",
    "section": "Descripción",
    "text": "Descripción\nSe analiza la temperatura en distintas estaciones de Texas durante el día 24/04/2024. A continuación se realiza el análisis.\n\n\nCode\nStations &lt;- read.csv(\"temp_stations_NA.csv\", check.names = FALSE)\nStations_sf &lt;- st_as_sf(Stations,\n                        coords = c(\"X\", \"Y\"),\n                        crs = 3083)\n\ntemp.data &lt;- read.csv(\"tempdata.csv\", check.names = FALSE)\n\n\n\nTemp_map &lt;- Stations_sf %&gt;%\n  left_join(temp.data, by = \"AQSID\") %&gt;% \n  na.omit()\n\nmapview(Temp_map, \n        zcol = \"Temp\", \n        legend = TRUE, \n        cex = 7, \n        col.regions = viridisLite::viridis, \n        popup = Temp_map$AQSID,\n        layer.name = \"°C\")"
  },
  {
    "objectID": "geostatistics/temp.html#análisis-de-estacionariedad",
    "href": "geostatistics/temp.html#análisis-de-estacionariedad",
    "title": "Análisis de la Temperatura",
    "section": "Análisis de Estacionariedad",
    "text": "Análisis de Estacionariedad\nPrimero, analizamos si se satisface la estacionariedad con respecto a la media. Para esto, nos apoyamos en los siguientes gráficos:\n\n\n\n\n\n\n\n\n\nComo se puede ver, no hay estacionariedad. Por lo tanto, se ajusta un modelo de regresión, de forma que los residuales presenten media 0 y se comporte de forma constante.\nEl mejor modelo que se ajustó tiene la siguiente forma:\n\\[\n\\text{temp}_i = \\beta_0 + \\beta_1\\cdot x + \\beta_2 \\cdot y + \\beta_3 \\cdot y^2\n\\]\n\n\nCode\nfit.temp &lt;- lm(Temp ~ X + Y + I(X^2) + I(X^3) + I(Y^3), data = temp.data)\nsummary(fit.temp)\n\n\n\nCall:\nlm(formula = Temp ~ X + Y + I(X^2) + I(X^3) + I(Y^3), data = temp.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1534 -0.6908 -0.0129  0.6860  3.0235 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.879e+02  7.389e+01  -3.896 0.000153 ***\nX            2.830e-04  2.317e-05  12.216  &lt; 2e-16 ***\nY            4.024e-05  1.422e-05   2.831 0.005357 ** \nI(X^2)      -1.894e-10  1.549e-11 -12.227  &lt; 2e-16 ***\nI(X^3)       3.953e-17  3.326e-18  11.887  &lt; 2e-16 ***\nI(Y^3)      -2.718e-19  8.644e-20  -3.144 0.002053 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.172 on 134 degrees of freedom\nMultiple R-squared:  0.8054,    Adjusted R-squared:  0.7981 \nF-statistic: 110.9 on 5 and 134 DF,  p-value: &lt; 2.2e-16\n\n\nComo se puede ver, se tiene un \\(R^2\\) del casi 90%. También veamos el gráfico de los valores ajustados vs los residuales.\n\n\n\n\n\n\n\n\n\nObteniendo, de esta forma, los siguientes gráficos descriptivos:\n\n\nCode\ntemp.data$Residuals &lt;- resid(fit.temp)\n\ntemp.Data &lt;- as.data.frame(temp.data)\n\ntempg &lt;- as.geodata(temp.data, \n                    coords.col = c(\"X\", \"Y\"), \n                    data.col = \"Residuals\")\nplot(tempg)\n\n\n\n\n\n\n\n\n\n\nSemivariograma\nA continuación se presenta la estimación empírica del semivariograma:\n\n\nCode\ncoordinates(temp.data) &lt;- ~ X + Y\n\nmaxdist &lt;- max(dist(coordinates(temp.data)))\n\ncutoff &lt;- 1/3*maxdist; width &lt;- cutoff/15\n\nvgm &lt;- variogram(Temp ~ X + Y + I(X^2) + I(X^3) + I(Y^3), \n                 data = temp.data,\n                 cutoff = cutoff, width = width)\n\nvg &lt;- variog(tempg, uvec = seq(vgm$dist[1], cutoff, by= width))\n\n\nvariog: computing omnidirectional variogram\n\n\nCode\nplot(vgm, ylab = \"Semivarianza\", xlab = \"Distancia\",\n     pch = 19, main = \"Semivariograma Empírico (Temperatura)\")\n\n\n\n\n\n\n\n\n\nDe este semivariograma podemos ver que se tiene un nugget pequeño. Además, la estructura de dependencia espacial es evidente: la semivarianza aumenta con la distancia, lo que sugiere que las estaciones cercanas presentan temperaturas más similares entre sí que las estaciones más alejadas.\n\nEstimación del Semivariograma Teórico\nComo un ajuste inicial, se usa la función eyefit de geoR para poder obtener unos buenos valores iniciales para los modelos que más se ajustan al semivariograma empírico. Se definen las funciones que se usarán para esto:\n\n\nCode\ngamma_sph &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,\n         ifelse(h &lt;= a,\n                c0 + cs * ( 1.5*(h/a) - 0.5*(h/a)^3 ),\n                c0 + cs))\n}\n\n\ngamma_exp &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,\n         c0 + cs * (1 - exp(-(h/a))))\n}\n\n\ngamma_gaus &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,\n         c0 + cs*(1 - exp(-(h/a)^2)))\n}\n\n\ngamma_mat &lt;- function(h, c0, cs, a, kappa = 1.35){\n  ifelse(h == 0,\n         0,\n         c0 + cs * (1 - ((h / a)^kappa * besselK(h / a, nu = kappa)) /\n                      (2^(kappa - 1) * gamma(kappa)))\n  )\n}\n\n\ngamma_wave &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,  \n         c0 + cs * (1 - (sin(h/a) / (h/a))))\n}\n\n\nMSE &lt;- function(fitted, observed){\n  \n  mean((fitted - observed)^2)\n  \n}\n\ndata &lt;- data.frame(h = vg$u,\n                   gamma_hat = vg$v,\n                   n = vg$n)\n\n\nY con los valores iniciales\n\n# CUBIC\ninit_cub &lt;- c(c0 = 0.28, cs = 1.10, a = 210000)\n\n# EXPONENTIAL\ninit_exp &lt;- c(c0 = 0.28, cs = 1.18, a = 81081.1)\n\n# GAUSSIAN\ninit_gau &lt;- c(c0 = 0.28, cs = 1.26, a = 97297.3)\n\n# MATERN \ninit_mat &lt;- c(c0 = 0.28, cs = 1.34, a = 64864.86, kappa = 1.35)\n\n# SPHERICAL\ninit_sph &lt;- c(c0 = 0.28, cs = 1.20, a = 162162.16)\n\n# WAVE\ninit_wav &lt;- c(c0 = 0.28, cs = 1.19, a = 32214)\n\nSe grafican estos ajustes “a ojo” iniciales.\n\n\nCode\nfit.sph_1 &lt;- gamma_sph(h = data$h, c0 = init_sph[1], cs = init_sph[2], a = init_sph[3])\nmse_1 &lt;- mean((fit.sph_1 - data$gamma_hat)^2)\n\nfit.exp_1 &lt;- gamma_exp(h = data$h, c0 = init_exp[1], cs = init_exp[2], a = init_exp[3])\nmse_2 &lt;- mean((fit.exp_1 - data$gamma_hat)^2)\n\nfit.gaus_1 &lt;- gamma_gaus(h = data$h, c0 = init_gau[1], cs = init_gau[2], a = init_gau[3])\nmse_3 &lt;- mean((fit.gaus_1 - data$gamma_hat)^2)\n\nfit.mat_1 &lt;- gamma_mat(h = data$h, c0 = init_mat[1], cs = init_mat[2], a = init_mat[3], kappa = init_mat[4])\nmse_4 &lt;- mean((fit.mat_1 - data$gamma_hat)^2)\n\nfit.wave_1 &lt;- gamma_wave(h = data$h, c0 = init_wav[1], cs = init_wav[2], a = init_wav[3])\nmse_5 &lt;- mean((fit.wave_1 - data$gamma_hat)^2)\n\n\n\ncolors &lt;- c(\n  \"dodgerblue3\",   # esférico\n  \"darkorange2\",   # exponencial\n  \"forestgreen\",   # gaussiano\n  \"orchid3\",       # matern\n  \"firebrick2\"     # wave\n)\n\n\nplot(x = data$h, y = data$gamma_hat, pch = 19,\n     xlab = \"Distancia\",\n     ylab = \"Semivarianza\",\n     main = \"Estimación teórica\")\n\nlines(data$h, fit.sph_1,  col = colors[1], lwd = 2)\nlines(data$h, fit.exp_1,  col = colors[2], lwd = 2)\nlines(data$h, fit.gaus_1, col = colors[3], lwd = 2)\nlines(data$h, fit.mat_1,  col = colors[4], lwd = 2)\nlines(data$h, fit.wave_1, col = colors[5], lwd = 2)\n\nlegend(\"topleft\",\n       legend = c(\"Esférico\", \"Exponencial\", \"Gausiano\", \"Matérn\", \"Wave\"),\n       col    = colors,\n       lwd    = 2,\n       pch    = NA,\n       bty    = \"n\")\n\n\n\n\n\n\n\n\n\n\nRegresión No Lineal\nPrimero, realizamos un ajuste mediante las funciones nls (base) y nlsLM del paquete minpack.lm. Para cada modelo se tuvieron en cuenta los siguientes pesos:\n\\[\n1, \\qquad n, \\qquad \\frac{n}{h^2}\n\\]\n\n\nCode\nnls.sph1 &lt;- nls(gamma_hat ~ gamma_sph(h, c0, cs, a),\n                data = data,\n                start = as.list(init_sph))\n\nnls.sph2 &lt;- nls(gamma_hat ~ gamma_sph(h, c0, cs, a),\n                data = data,\n                start = as.list(init_sph), \n                weights = n)\n\n\nnls.sph3 &lt;- nlsLM(gamma_hat ~ gamma_sph(h, c0, cs, a),\n                  data = data, \n                  start = as.list(init_sph),\n                  weights = (n/h^2),\n                  control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n\nnls.exp1 &lt;- nls(gamma_hat ~ gamma_exp(h, c0, cs, a),\n                data = data,\n                start = as.list(init_exp), \n                algorithm = \"port\",\n                lower = c(0, 0, 0))\n\nnls.exp2 &lt;- nls(gamma_hat ~ gamma_exp(h, c0, cs, a),\n                data = data,\n                start = as.list(init_exp), \n                algorithm = \"port\",\n                lower = c(0, 0, 0), \n                weights = n)\n\nnls.exp3 &lt;- nlsLM(gamma_hat ~ gamma_exp(h, c0, cs, a),\n                  data = data, \n                  start = as.list(init_exp),\n                  weights = (n/h^2),\n                  control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n\n\nnls.gaus1 &lt;- nls(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_gau))\n\nctrl &lt;- nls.control(maxiter = 1000, tol = 1e-10, minFactor = 1/1024)\nnls.gaus2 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                  data = data,\n                  start = as.list(init_gau),\n                  weights = n,\n                  control = ctrl)\n\nnls.gaus3 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data, \n                   start = as.list(init_gau),\n                   weights = (n/h^2),\n                   control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n\n\nnls.mat1 &lt;- nls(gamma_hat ~ gamma_mat(h, c0, cs, a, kappa = 1.35),\n                data = data,\n                start = as.list(init_mat[-4]))\n\nnls.mat2 &lt;- nls(gamma_hat ~ gamma_mat(h, c0, cs, a, kappa = 1.35),\n                data = data,\n                start = as.list(init_mat[-4]), \n                weights = n)\n\nnls.mat3 &lt;- nlsLM(gamma_hat ~ gamma_mat(h, c0, cs, a, kappa = 1.35),\n                  data = data, \n                  start = as.list(init_mat[-4]),\n                  weights = (n/h^2),\n                  control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n\n\nnls.wave1 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav))\n\nnls.wave2 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 weights = n)\n\nnls.wave3 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 algorithm = \"port\",\n                 lower = c(0, 0, 0), \n                 weights = (n/h^2))\n\n\nDe esta forma, se obtienen los siguientes resultados:\n\n\n\nMSE para cada modelo y peso\n\n\nModelo\nMSE\nPeso\n\n\n\n\nEsférico\n0.0300\n\\(1\\)\n\n\nEsférico\n0.0309\n\\(n\\)\n\n\nEsférico\n0.0304\n\\(\\frac{n}{h^2}\\)\n\n\nExponencial\n0.0372\n\\(1\\)\n\n\nExponencial\n0.0385\n\\(n\\)\n\n\nExponencial\n0.0434\n\\(\\frac{n}{h^2}\\)\n\n\nGaussiano\n0.0320\n\\(1\\)\n\n\nGaussiano\n0.0330\n\\(n\\)\n\n\nGaussiano\n0.0329\n\\(\\frac{n}{h^2}\\)\n\n\nMátern\n0.0346\n\\(1\\)\n\n\nMátern\n0.0358\n\\(n\\)\n\n\nMátern\n0.0359\n\\(\\frac{n}{h^2}\\)\n\n\nWave\n0.0281\n\\(1\\)\n\n\nWave\n0.0481\n\\(n\\)\n\n\nWave\n0.0368\n\\(\\frac{n}{h^2}\\)\n\n\n\n\n\nEl mejor por modelo se presenta a continuación:\n\n\n\nMejor combinación modelo–peso\n\n\nModelo\nMSE\nPeso\n\n\n\n\nEsférico\n0.0300\n\\(1\\)\n\n\nExponencial\n0.0372\n\\(1\\)\n\n\nGaussiano\n0.0320\n\\(1\\)\n\n\nMátern\n0.0346\n\\(1\\)\n\n\nWave\n0.0281\n\\(1\\)\n\n\n\n\n\nPor este método, el mejor ajuste que se obtuvo fue con el modelo Wave con parámetros al obtener un menor MSE.\nVeamos gráficamente como se ven estos modelos:\n\n\n\n\n\n\n\n\n\n\n\nCressie\nAhora se considera un método en el cual la matriz de ponderaciones cambia en cada iteración. Esto se desarrollará mediante las siguientes funciones:\n\n\nCode\nQ_cressie &lt;- function(par, data, gamma_fun, kappa = NULL, eps = 1e-12){\n  \n  g &lt;- if (is.null(kappa)) {\n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"])\n    \n  } else {\n    \n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"], kappa = kappa)\n    \n  }\n  \n  g2 &lt;- pmax(g^2, eps)         \n  \n  sum( data$n * (data$gamma_hat - g)^2 / (2 * g2), na.rm = TRUE )\n}\n\n\nfit_cressie &lt;- function(gamma_fun, start, data,\n                        lower = c(c0=0, cs=0, a=.Machine$double.eps),\n                        upper = c(c0=Inf, cs=Inf, a=Inf),\n                        kappa = NULL){\n  \n  par0 &lt;- start[c(\"c0\",\"cs\",\"a\")]    \n  \n  opt &lt;- optim(par = par0, fn = Q_cressie, method = \"L-BFGS-B\",\n               lower = lower, upper = upper,\n               data = data, gamma_fun = gamma_fun, kappa = kappa)\n  \n  \n  p  &lt;- opt$par\n  \n  g  &lt;- if (is.null(kappa)) {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"])\n  } else {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"], kappa)\n  }    \n  \n  mse &lt;- MSE(g, data$gamma_hat)\n  \n  list(par = p, converged = (opt$convergence==0),\n       fitted = g, MSE = mse, kappa = kappa)\n}\n\n\nDe esta forma, se realiza el ajuste para cada modelo.\n\n\nCode\ncress.sph &lt;- fit_cressie(gamma_sph, start = init_sph, data = data)\n\n\ncress.exp &lt;- fit_cressie(gamma_exp, start = init_exp, data = data)\n\n\ncress.gaus &lt;- fit_cressie(gamma_gaus, start = init_gau, data = data)\n\n\ncress.mat &lt;- fit_cressie(gamma_mat, start = init_mat, data = data, kappa = 1.35)\n\n\ncress.wave &lt;- fit_cressie(gamma_wave, start = init_wav, data = data)\n\n\nObteniendo los siguientes resultados:\n\nord &lt;- order(data$h)\n\nplot(data$h, data$gamma_hat,\n     pch = 16, cex = 0.7,\n     xlab = \"Distancia h\",\n     ylab = expression(hat(gamma)(h)),\n     main = \"Variograma Empírico y Ajustes (Cressie)\")\n\nlines(data$h[ord], cress.sph$fitted[ord],  col = \"red\",       lwd = 2)\nlines(data$h[ord], cress.exp$fitted[ord],  col = \"blue\",      lwd = 2)\nlines(data$h[ord], cress.gaus$fitted[ord], col = \"darkgreen\", lwd = 2)\nlines(data$h[ord], cress.mat$fitted[ord],  col = \"purple\",    lwd = 2)\n\nlegend(\"bottomright\",\n       legend = c(\"Esférico\",\n                  \"Exponencial\",\n                  \"Gaussiano\",\n                  \"Matérn\"),\n       col    = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lty    = 1.5,\n       lwd    = 2,\n       bty    = \"n\")\n\n\n\n\n\n\n\n\n\n\n\nMSE para ajustes Cressie\n\n\nModelo\nMSE\n\n\n\n\nsph\n0.0318\n\n\nexp\n0.0399\n\n\ngaus\n0.0325\n\n\nmat\n0.0492\n\n\nwave\n0.0455\n\n\n\n\n\nDe esta forma, es posible ver que el menor error cuadrático medio lo tiene el modelo esférico. De todas formas, no es menor que el obtenido por regresión no lineal.\n\n\nMáxima Verosimilitud\nAl considerar este método es necesario expresar las matrices de covarianza en función de los parámetros que deseamos estimar. Esto es:\nEntonces, se encuentran las funciones de covarianza para cada modelo:\n\n\nCode\ncov.gaus &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         c0 + cs,\n         cs * exp(-(h/a)^2))\n}\n\ncov.exp &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         c0 + cs,\n         cs * exp(-h/a))\n}\n\ncov.sph &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         c0 + cs,\n         ifelse(h &gt; 0 & h &lt; a,\n                cs * (1 - (3/2)*(h/a) + (1/2)*(h/a)^3),\n                0))\n}\n\ncov.mat &lt;- function(h, c0, cs, a, kappa = 1.35){\n  ifelse(h == 0,\n         c0 + cs,\n         cs * (1 - (1 - ((h / a)^kappa * besselK(h / a, nu = kappa)) /\n                 (2^(kappa - 1) * gamma(kappa)))))\n}\n\ncov.wave &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         c0 + cs,\n         cs * (sin(h/a)/(h/a)))\n}\n\n\nY además, la función de log-verosimilitud explícita para poder realizar la optimización con la ayuda de optim.\n\n\nCode\nloglik.model &lt;- function(par, Z, coords, cov.model) {\n  c0 &lt;- par[1]  \n  cs &lt;- par[2]   \n  a  &lt;- par[3]   \n  \n  \n  h &lt;- as.matrix(dist(coords))\n  \n  \n  Sigma &lt;- cov.model(h, c0, cs, a)\n  \n  \n  n &lt;- length(Z)\n  L &lt;- try(chol(Sigma), silent = TRUE)\n  if (inherits(L, \"try-error\")) return(1e6)\n  \n  \n  SinvZ &lt;- backsolve(L, forwardsolve(t(L), Z))\n  \n  logdet &lt;- 2 * sum(log(diag(L)))\n  nll &lt;- 0.5 * (n * log(2*pi) + logdet + sum(Z * SinvZ))\n  \n  return(nll)  \n}\n\n\nDe esta forma, se realiza el ajuste para los modelos:\n\n\nCode\nmv.sph &lt;- optim(\n  par = init_sph,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.sph,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.sph &lt;- within(mv.sph, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_sph(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_sph(data$h, c0, cs, a))\n})\n\n\nmv.exp &lt;- optim(\n  par = init_exp,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.exp,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.exp &lt;- within(mv.exp, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_exp(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_exp(data$h, c0, cs, a))\n})\n\n\nmv.gaus &lt;- optim(\n  par = init_gau,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.gaus,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.gaus &lt;- within(mv.gaus, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_gaus(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_gaus(data$h, c0, cs, a))\n})\n\n\nmv.mat &lt;- optim(\n  par = init_mat,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.mat,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.mat &lt;- within(mv.mat, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_mat(data$h, c0, cs, a, kappa = 1.35), \n                                       data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_mat(data$h, c0, cs, a, kappa = 1.35))\n})\n\n\nmv.wave &lt;- optim(\n  par = init_wav,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.wave,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0),\n  upper = c(Inf, Inf, Inf)\n)\n\nmv.wave &lt;- within(mv.wave, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_wave(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_wave(data$h, c0, cs, a))\n})\n\n\nObteniendo los siguientes resultados:\n\n\n\n\n\n\n\n\n\n\n\n\nMSE para ajustes MV\n\n\nModelo\nMSE\n\n\n\n\nEsférico\n0.0340\n\n\nExponencial\n0.0403\n\n\nGausiano\n0.0780\n\n\nMatern\n0.1318\n\n\nWave\n0.1394\n\n\n\n\n\nVemos que el mejor modelo en este caso es el esférico. De igual forma que se había comentado con Cressie, este modelo sigue teniendo mayor MSE que el obtenido por regresión no lineal.\n\n\nMCO y MCP\nPrimero, se realiza la construcción de las funciones que se van a usar para la estimación por mínimos cuadrados.\n\n\nCode\nLS.model &lt;- function(par, data, gamma.model, w = c(\"1\", \"n\", \"n/h2\")){\n  c0 &lt;- par[1]  \n  cs &lt;- par[2]   \n  a  &lt;- par[3] \n  \n  h &lt;- data$h\n  gamma_hat  &lt;- data$gamma\n  n_pairs &lt;- data$n\n  \n  \n  gamma_theo &lt;- gamma.model(h, c0, cs, a)\n  \n  diff &lt;- gamma_hat - gamma_theo\n  \n  \n  w &lt;- match.arg(w)\n  w &lt;- switch(w,\n              \"1\"      = rep(1, length(h)),\n              \"n\"         = n_pairs,\n              \"n/h2\" = n_pairs / (h^2))\n  \n  Q &lt;- sum(w * diff^2, na.rm = TRUE)\n  \n  return(Q)\n}\n\nMSE_fit &lt;- function(par, gamma.model){\n  mean((data$gamma - gamma.model(data$h, par[1], par[2], par[3]))^2)\n}\n\n\n\nEsféricoExponencialGausianoMáternWave\n\n\n\n\nCode\nmc0.sph &lt;- optim(\n  par = init_sph,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_sph,\n  w = \"1\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\nmc1.sph &lt;- optim(\n  par = init_sph,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_sph,\n  w = \"n\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\nmc2.sph &lt;- optim(\n  par = init_sph,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_sph,\n  w = \"n/h2\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\n\n\n\n\n\nPeso\nMSE\n\n\n\n\n\\(1\\)\n0.0300\n\n\n\\(n\\)\n0.0309\n\n\n\\(\\frac{n}{h^2}\\)\n0.0322\n\n\n\n\n\n\n\n\n\nCode\nmc0.exp &lt;- optim(\n  par = init_exp,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_exp,\n  w = \"1\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\nmc1.exp &lt;- optim(\n  par = init_exp,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_exp,\n  w = \"n\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\nmc2.exp &lt;- optim(\n  par = init_exp,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_exp,\n  w = \"n/h2\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\n\n\n\n\n\nPeso\nMSE\n\n\n\n\n\\(1\\)\n0.0372\n\n\n\\(n\\)\n0.0385\n\n\n\\(\\frac{n}{h^2}\\)\n0.0481\n\n\n\n\n\n\n\n\n\nCode\nmc0.gau &lt;- optim(\n  par = init_gau,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_gaus,\n  w = \"1\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\nmc1.gau &lt;- optim(\n  par = init_gau,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_gaus,\n  w = \"n\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\nmc2.gau &lt;- optim(\n  par = init_gau,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_gaus,\n  w = \"n/h2\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\n\n\n\n\n\nPeso\nMSE\n\n\n\n\n\\(1\\)\n0.0320\n\n\n\\(n\\)\n0.0330\n\n\n\\(\\frac{n}{h^2}\\)\n0.0384\n\n\n\n\n\n\n\n\n\nCode\nmc0.mat &lt;- optim(\n  par = init_mat,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_mat,\n  w = \"1\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\nmc1.mat &lt;- optim(\n  par = init_mat,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_mat,\n  w = \"n\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\nmc2.mat &lt;- optim(\n  par = init_mat,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_mat,\n  w = \"n/h2\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\n\n\n\n\n\nPeso\nMSE\n\n\n\n\n\\(1\\)\n0.0346\n\n\n\\(n\\)\n0.0358\n\n\n\\(\\frac{n}{h^2}\\)\n0.0651\n\n\n\n\n\n\n\n\n\nCode\nmc0.wav &lt;- optim(\n  par = init_wav,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_wave,\n  w = \"1\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\nmc1.wav &lt;- optim(\n  par = init_wav,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_wave,\n  w = \"n\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\nmc2.wav &lt;- optim(\n  par = init_wav,\n  fn = LS.model,\n  data = data,\n  gamma.model = gamma_wave,\n  w = \"n/h2\",\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0))\n\n\n\n\n\n\n\nCode\nmc0.sph &lt;- optim(\n  par = init_sph,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.sph,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.sph &lt;- within(mv.sph, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_sph(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_sph(data$h, c0, cs, a))\n})\n\n\nmv.exp &lt;- optim(\n  par = init_exp,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.exp,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.exp &lt;- within(mv.exp, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_exp(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_exp(data$h, c0, cs, a))\n})\n\n\nmv.gaus &lt;- optim(\n  par = init_gau,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.gaus,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.gaus &lt;- within(mv.gaus, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_gaus(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_gaus(data$h, c0, cs, a))\n})\n\n\nmv.mat &lt;- optim(\n  par = init_mat,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.mat,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.mat &lt;- within(mv.mat, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_mat(data$h, c0, cs, a, kappa = 1.35), \n                                       data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_mat(data$h, c0, cs, a, kappa = 1.35))\n})\n\n\nmv.wave &lt;- optim(\n  par = init_wav,\n  fn = loglik.model,\n  Z = temp.data$Residuals,\n  coords = coordinates(temp.data),\n  cov.model = cov.wave,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0),\n  upper = c(Inf, Inf, Inf)\n)\n\nmv.wave &lt;- within(mv.wave, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_wave(data$h, c0, cs, a), data$gamma_hat))\n  gamma_pred &lt;- with(as.list(par), gamma_wave(data$h, c0, cs, a))\n})"
  },
  {
    "objectID": "geostatistics/temp.html#kriging",
    "href": "geostatistics/temp.html#kriging",
    "title": "Análisis de la Temperatura",
    "section": "Kriging",
    "text": "Kriging\nDespués de haber obtenido las estimaciones por los distintos métodos, se llega a la conclusión que el mejor modelo corresponde al Wave con los siguientes parámetros:\n\n\n\nParámetros Estimados\n\n\n\nEstimación\n\n\n\n\n\\(c_0\\)\n0.57358\n\n\n\\(c_s\\)\n0.91243\n\n\n\\(a\\)\n47992.04376\n\n\n\n\n\nDe esta forma, se define el objeto que almacena el modelo y se grafica:\n\n\nCode\nbest_par &lt;- coef(nls.wave1)\n\nbest_model &lt;- vgm(psill = best_par[\"cs\"], \n                  model = \"Wav\", \n                  range = 3*best_par[\"a\"], \n                  nugget = best_par[\"c0\"])\n\nplot(vgm, best_model,\n     xlab = \"Distancia\", ylab = \"Semivarianza\",\n     main = \"Estimación Teórica del Semivariograma\",\n     pch = 19, lwd = 1.5, col = \"dodgerblue3\")\n\n\n\n\n\n\n\n\n\nSe define la grilla para realizar la interpolación basada en este modelo.\n\n\nCode\ntx &lt;- states(cb = TRUE, year = 2023) |&gt;\n  st_as_sf() |&gt;\n  dplyr::filter(STUSPS == \"TX\") |&gt;\n  st_transform(3083)\n\ngrid_tx &lt;- st_make_grid(tx, cellsize = 10000, square = TRUE) %&gt;%   # 10 km de resolución\n  st_intersection(tx) %&gt;% \n  st_centroid() %&gt;% \n  st_as_sf()\n\ngrid &lt;- as.data.frame(st_coordinates(grid_tx))\n\ncoordinates(grid) &lt;- ~ X + Y\n\n\nY se realiza el kriging:\n\n\nCode\nkrige_temp &lt;- gstat::krige(\n  formula = Temp ~ X + Y + I(X^2) + I(X^3) + I(Y^3),\n  locations = temp.data,\n  newdata   = grid,\n  model     = best_model, \n  maxdist = 8e5)\n\n\n[using universal kriging]\n\n\nObteniendo los siguientes resultados:\n\nPredicciónError de Predicción\n\n\n\n\nCode\nStations_sp &lt;- as(Stations_sf, \"Spatial\")\n\nstations_layer &lt;- list(\n  \"sp.points\",\n  Stations_sp,\n  pch = 21,      \n  cex = 1.4,\n  bg = viridis(100),\n  col = \"black\")\n\n\n\n\n\nrng    &lt;- range(krige_temp$var1.pred, na.rm = TRUE)\n\nat_lab &lt;- pretty(rng, n = 10)          \n\nspplot(krige_temp, \"var1.pred\", colorkey = list(\n        right = list(\n          fun = draw.colorkey,\n          args = list(\n            key = list(\n              at = at_lab,\n              col = viridis(100),\n              labels = list(\n                at = at_lab\n              )\n            )\n          )\n        )\n      ), \n      col.regions = viridis(100),\n      sp.layout   = list(stations_layer),\n      main = \"Mapa de Predicción de la Temperatura °C\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrng    &lt;- range(krige_temp$var1.var, na.rm = TRUE)\n\nat_lab &lt;- pretty(rng, n = 10)  \n\nspplot(krige_temp, \"var1.var\", colorkey = list(\n        right = list(\n          fun = draw.colorkey,\n          args = list(\n            key = list(\n              at = at_lab,\n              col = magma(100),\n              labels = list(\n                at = at_lab\n              )\n            )\n          )\n        )\n      ), \n      col.regions = magma(100),\n      main = \"Error de Predicción de la Temperatura °C\")\n\n\n\n\n\n\n\n\n\n\n\n\nDe estos mapas es posible concluir que el oriente de Texas es el lugar donde se obtienen las temperaturas más bajas en comparación con las demás. En el oriente y, más especialmente el suroccidente, presenta las temperaturas más altas llegando incluso a 38 grados centígrados aproxidamente. Con respecto al mapa del error de predicción, como es de esperarse, se presentan errores más altos en los lugares donde no se tienen observaciones, mientras que hay un error menor donde hay bastante presencia de estaciones.\nA continuación, se presenta la validación cruzada del kriging (LOOCV) con el modelo escogido.\n\nkrige_temp_cv &lt;- krige.cv(formula = Temp ~ X + Y + I(X^2) + I(X^3) + I(Y^3),\n                          locations = temp.data,\n                          model     = best_model, \n                          maxdist = 8e5)\n\nY se obtienen las siguientes medidas de evaluación:\n\nmetricas_cv &lt;- function(cv) {\n  data.frame(\n    ME = mean(cv$residual),\n    MAE = mean(abs(cv$residual)),\n    RMSE = sqrt(mean(cv$residual^2)),\n    R2 = 1 - var(cv$residual) / var(cv$observed),\n    MPSE = mean(cv@data$residual^2 / cv@data$var1.var),\n    COR = cor(cv@data$var1.pred, cv@data$observed)\n  )\n}\n\nmetricas_cv(krige_temp_cv)\n\n          ME       MAE      RMSE        R2      MPSE       COR\n1 0.02341561 0.6204751 0.9182373 0.8752771 0.9339834 0.9357251\n\n\nEn conjunto, estos resultados indican un buen desempeño del modelo tanto en términos de precisión como de consistencia. En primer lugar, el Error Medio (ME) es cercano a cero, lo que sugiere que el modelo no tiende a sobreestimar ni subestimar las observaciones.\nEl MAE y el RMSE, con valores de aproximadamente 6 y 9 unidades respectivamente, muestran que la magnitud de los errores es razonable para la escala de la variable.\nAdemás, el \\(R^2\\) indica que se está explicando un gran porcentaje de la variabilidad observada. De igual forma la correlación entre los valores observados y los predichos en las estaciones es alta, indicando que se están logrando predicciones de muy buena calidad."
  },
  {
    "objectID": "geostatistics/ozone.html",
    "href": "geostatistics/ozone.html",
    "title": "Análisis del Ozono",
    "section": "",
    "text": "Se muestra el mapa de la distribución espacial de las estaciones en el estado de Texas, Estados Unidos.\n\n\nCode\nOZ &lt;- read.csv(\"G:/Mi unidad/Camilo UNAL/Estadística Espacial/Proyecto/Datos/TexasOZone2024.csv\")\n\nOZ_stations &lt;- OZ %&gt;% \n  distinct(Site.ID, .keep_all = TRUE) %&gt;% \n  dplyr::select(Site.ID, Site.Longitude, Site.Latitude) %&gt;% \n  rename(ID = Site.ID)\n\n\ncoords_sf &lt;- st_as_sf(OZ_stations, coords = c(\"Site.Longitude\", \"Site.Latitude\"), crs = 4326)\n\ncrs_texas &lt;- 3083\n\ncoords_proj &lt;- st_transform(coords_sf, crs = crs_texas)\n\n## Coordenadas planas ----\n\ntexas_coords &lt;- st_coordinates(coords_proj)\n\nOZ_stations$X &lt;- texas_coords[,1]\nOZ_stations$Y &lt;- texas_coords[,2]\n\n\n\n\nOZ &lt;- read.csv(\"G:/Mi unidad/Camilo UNAL/Estadística Espacial/Proyecto/Primera Entrega/OZ.csv\", check.names = FALSE)[,-1]\n\n## Mapa con Datos Ozono ----\n\nOZ_map &lt;- coords_proj %&gt;%\n  left_join(OZ, by = \"ID\") %&gt;% \n  na.omit()\n\nmapview(OZ_map, \n        zcol = \"Ozone\", \n        legend = TRUE, \n        cex = 7, \n        col.regions = viridisLite::viridis, \n        popup = OZ_map$Site.ID,\n        layer.name = \"Ozono (ppb)\")\n\n\n\n\n\n\nComo se puede apreciar, no hay muchas estaciones hacia el noroccidente de Texas. Además de esto, se muestran los valores de Ozono."
  },
  {
    "objectID": "geostatistics/ozone.html#descripción",
    "href": "geostatistics/ozone.html#descripción",
    "title": "Análisis del Ozono",
    "section": "",
    "text": "Se muestra el mapa de la distribución espacial de las estaciones en el estado de Texas, Estados Unidos.\n\n\nCode\nOZ &lt;- read.csv(\"G:/Mi unidad/Camilo UNAL/Estadística Espacial/Proyecto/Datos/TexasOZone2024.csv\")\n\nOZ_stations &lt;- OZ %&gt;% \n  distinct(Site.ID, .keep_all = TRUE) %&gt;% \n  dplyr::select(Site.ID, Site.Longitude, Site.Latitude) %&gt;% \n  rename(ID = Site.ID)\n\n\ncoords_sf &lt;- st_as_sf(OZ_stations, coords = c(\"Site.Longitude\", \"Site.Latitude\"), crs = 4326)\n\ncrs_texas &lt;- 3083\n\ncoords_proj &lt;- st_transform(coords_sf, crs = crs_texas)\n\n## Coordenadas planas ----\n\ntexas_coords &lt;- st_coordinates(coords_proj)\n\nOZ_stations$X &lt;- texas_coords[,1]\nOZ_stations$Y &lt;- texas_coords[,2]\n\n\n\n\nOZ &lt;- read.csv(\"G:/Mi unidad/Camilo UNAL/Estadística Espacial/Proyecto/Primera Entrega/OZ.csv\", check.names = FALSE)[,-1]\n\n## Mapa con Datos Ozono ----\n\nOZ_map &lt;- coords_proj %&gt;%\n  left_join(OZ, by = \"ID\") %&gt;% \n  na.omit()\n\nmapview(OZ_map, \n        zcol = \"Ozone\", \n        legend = TRUE, \n        cex = 7, \n        col.regions = viridisLite::viridis, \n        popup = OZ_map$Site.ID,\n        layer.name = \"Ozono (ppb)\")\n\n\n\n\n\n\nComo se puede apreciar, no hay muchas estaciones hacia el noroccidente de Texas. Además de esto, se muestran los valores de Ozono."
  },
  {
    "objectID": "geostatistics/ozone.html#análisis-de-estacionariedad",
    "href": "geostatistics/ozone.html#análisis-de-estacionariedad",
    "title": "Análisis del Ozono",
    "section": "Análisis de Estacionariedad",
    "text": "Análisis de Estacionariedad\nA continuación se presenta el resúmen visual de los datos de Ozono en Texas. De igual forma, también se muestra el gráfico del Ozono vs Coordenadas.\n\nOZ$OzC &lt;- residuals(lm(Ozone ~ 1, data = OZ))\n\nOZg &lt;- as.geodata(OZ,\n                  coords.col = 2:3,\n                  data.col = 5)\n\nplot(OZg)\n\n\n\n\n\n\n\n\n\npar(mfrow = c(2, 1), mar = c(2.75, 3, 0.5, 0.5), mgp = c(1.7, 0.7, 0))\n\nplot(x = OZ$Ozone, y = OZ$Northing,\n     xlab = \"Ozono\", ylab = \"Norte\",\n     col = \"lightblue3\")\n\nplot(x = OZ$Easting, y = OZ$Ozone,\n     xlab = \"Este\", ylab = \"Ozono\",\n     col = \"tomato\")\n\n\n\n\n\n\n\n\nDe estos gráficos es posible ver que la variable presenta estacionariedad con respecto a la media y, además, no se evidencian problemas con respecto a heteroscedasticidad. Por lo tanto, no es necesario ajustar un modelo lineal para intentar alcanzar estacionariedad."
  },
  {
    "objectID": "geostatistics/ozone.html#semivariograma",
    "href": "geostatistics/ozone.html#semivariograma",
    "title": "Análisis del Ozono",
    "section": "Semivariograma",
    "text": "Semivariograma\nA pesar de haber ajustado el modelo lineal, el Ozono en Texas parece ser estacionario desde un comienzo. De esta forma, se halla el semivariograma empírico para estos dos casos.\n\nDatos originales\n\n\nvariog: computing omnidirectional variogram\n\n\n\n\n\n\n\n\n\nComo se espera, a puntos cercanos hay menor variabilidad en cuanto a la semivarianza. Se puede ver que se presenta un efecto pepita pequeño, pues la semivarianza no pasa por el orígen. Además, se puede ver que el rango ocurre en aproximadamente \\(400.000\\) unidades de distancia.\n\n\nResistente a datos atípicos\n\n\nvariog: computing omnidirectional variogram\n\n\n\n\n\n\n\n\n\nEs posible ver que es similar al semivariograma basado en el promedio. Esto se puede estar dando debido a que no hay presencia de datos atípicos que estén afectando el nivel medio de la variable.\n\n\nEstimación teórica del semivariograma\nCon base en la sección de Modelos válidos para el semivariograma, vemos que los que más se asimilan al semivariograma empírico son el Gaussiano, Cúbico y Matern.\nPor lo tanto, con ayuda de la función eyefit, inicialmente ajustamos a “ojo” los parámetros del modelo gaussiano que se ajusta a los datos de Ozono:\n\n\nCode\ninit_gau &lt;- c(c0 = 10, cs = 34.88, a = 9e+05)\ninit_mat &lt;- c(c0 = 10, cs = 46.5, a = 1036220.5, kappa = 0.7)\ninit_cub &lt;- c(c0 = 10, cs = 28.42, a = 1727035.38)\n\n\nEntonces, los semivariogramas con estos parámetros iniciales se ven de la siguiente forma:\n\n\nCode\ngamma_gaus &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,\n         c0 + cs*(1 - exp(-(h/a)^2)))\n}\n\n\ngamma_mat &lt;- function(h, c0, cs, a, kappa){\n  ifelse(h == 0,\n         0,\n         c0 + cs * (1 - ((h / a)^kappa * besselK(h / a, nu = kappa)) /\n                      (2^(kappa - 1) * gamma(kappa)))\n  )\n}\n\ngamma_cub &lt;- function(h, c0, cs, a) {\n  ifelse(\n    h == 0, \n    0,\n    ifelse(\n      h &lt;= a,  \n      c0 + cs * (7*(h/a)^2 - 8.75*(h/a)^3 + 3.5*(h/a)^5 - 0.75*(h/a)^7),\n      c0 + cs   \n    )\n  )\n}\n\nMSE &lt;- function(fitted, observed){\n  \n  mean((fitted - observed)^2)\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\nRegresión No Lineal\nAhora, con los valores iniciales escogidos anteriormente, realizamos la estimación de los parámetros mediante regresión no lineal.\n\n\nCode\nlibrary(minpack.lm)\n\ndata_OZ &lt;- data.frame(h = vargrm1$u,\n                      gamma_est = vargrm1$v,\n                      n = vargrm1$n)\n\n\n# Gausiano\nnls.gaus1 &lt;- nls(gamma_est ~ gamma_gaus(h, c0, cs, a),\n                 data = data_OZ,\n                 start = as.list(init_gau))\n\nnls.gaus2 &lt;- nls(gamma_est ~ gamma_gaus(h, c0, cs, a),\n                  data = data_OZ,\n                  start = as.list(init_gau),\n                  weights = n)\n\nnls.gaus3 &lt;- nlsLM(gamma_est ~ gamma_gaus(h, c0, cs, a),\n                   data = data_OZ, \n                   start = as.list(init_gau),\n                   weights = (n/h^2),\n                   control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n\n# Cúbico\n\nnls.cub1 &lt;- nls(gamma_est ~ gamma_cub(h, c0, cs, a),\n                data = data_OZ,\n                start = as.list(init_cub))\n\nnls.cub2 &lt;- nls(gamma_est ~ gamma_cub(h, c0, cs, a),\n                data = data_OZ,\n                start = as.list(init_cub), \n                weights = n)\n\nnls.cub3 &lt;- nlsLM(gamma_est ~ gamma_cub(h, c0, cs, a),\n                  data = data_OZ, \n                  start = as.list(init_cub),\n                  weights = (n/h^2),\n                  control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n# Mátern\n\nnls.mat1 &lt;- nls(gamma_est ~ gamma_mat(h, c0, cs, a, kappa = 0.7),\n                data = data_OZ,\n                start = as.list(init_mat[-4]))\n\nnls.mat2 &lt;- nlsLM(gamma_est ~ gamma_mat(h, c0, cs, a, kappa = 0.7),\n                 data = data_OZ,\n                 start = as.list(init_mat[-4]), \n                 weights = n,\n                 control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\nnls.mat3 &lt;- nlsLM(gamma_est ~ gamma_mat(h, c0, cs, a, kappa = 0.7),\n                  data = data_OZ, \n                  start = as.list(init_mat[-4]),\n                  weights = (n/h^2),\n                  control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10))\n\n\n\n\n\n\n\nModelo\nMSE\nPeso\n\n\n\n\nGausiano\n20.0814\n\\(1\\)\n\n\nGausiano\n26.8063\n\\(n\\)\n\n\nGausiano\n30.9100\n\\(\\frac{n}{h^2}\\)\n\n\nCúbico\n20.4941\n\\(1\\)\n\n\nCúbico\n28.4781\n\\(n\\)\n\n\nCúbico\n30.8811\n\\(\\frac{n}{h^2}\\)\n\n\nMátern\n23.2330\n\\(1\\)\n\n\nMátern\n25.1138\n\\(n\\)\n\n\nMátern\n40.6334\n\\(\\frac{n}{h^2}\\)\n\n\n\n\n\nAdemás de comparar las gráficas de los mejores\n\nGaussianoCúbicoMátern\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTanto con la gráfica como con el MSE, nos damos cuenta que el modelo no lineal en el que no se considera una matriz de ponderación se ajusta de mejor forma al semivariograma empírico. Se presentan los coeficientes estimados por este método:\n\n\nWarning in kable_styling(., position = \"center\", full_width = FALSE): Please\nspecify format in kable. kableExtra can customize either HTML or LaTeX outputs.\nSee https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nMejor modelo no lineal\n\n\nParámetro\nEstimación\n\n\n\n\n\\(c_0\\)\n9.852\n\n\n\\(c_s\\)\n36.0693\n\n\n\\(a\\)\n845089.1454\n\n\n\n\n\n\n\nCressie\nSe define la función que nos ayudará a realizar la estimación de los parámetros de los distintos modelos.\n\n\nCode\nQ_cressie &lt;- function(par, data, gamma_fun, kappa = NULL, eps = 1e-12){\n  \n  g &lt;- if (is.null(kappa)) {\n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"])\n    \n  } else {\n    \n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"], kappa = kappa)\n    \n  }\n  \n  g2 &lt;- pmax(g^2, eps)         \n  \n  sum( data$n * (data$gamma_hat - g)^2 / (2 * g2), na.rm = TRUE )\n}\n\n\nfit_cressie &lt;- function(gamma_fun, start, data,\n                        lower = c(c0=0, cs=0, a=.Machine$double.eps),\n                        upper = c(c0=Inf, cs=Inf, a=Inf),\n                        kappa = NULL){\n  \n  par0 &lt;- start[c(\"c0\",\"cs\",\"a\")]    \n  \n  opt &lt;- optim(par = par0, fn = Q_cressie, method = \"L-BFGS-B\",\n               lower = lower, upper = upper,\n               data = data, gamma_fun = gamma_fun, kappa = kappa)\n  \n  \n  p  &lt;- opt$par\n  \n  g  &lt;- if (is.null(kappa)) {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"])\n  } else {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"], kappa)\n  }    \n  \n  mse &lt;- MSE(g, data$gamma_est)\n  \n  list(par = p, converged = (opt$convergence==0),\n       fitted = g, MSE = mse, kappa = kappa)\n}\n\n\nDe esta forma, aplicamos la función a cada modelo haciendo uso, además, de optim. Vea que en este caso, la matriz de ponderaciones cambia con cada iteración.\nLos resultados se presentan a continuación:\n\n\n\n\n\nModelo\nMSE\n\n\n\n\nCúbico\n178.8011\n\n\nGausiano\n22.7620\n\n\nMátern\n28.8839\n\n\n\n\n\nComo es posible apreciar, de nuevo el modelo Gausiano nos muestra un mejor ajuste. Aún así, el ajuste obtenido por nls fue mejor.\n\n\nMáxima Verosimilitud\nPrimero, es importante considerar las funciones de Covarianza de los modelos que se están considerando para la variable de Ozono. Además de esto, también se define una función general que permite usar la función de log-verosimilitud en general para poder realizar la optimización.\n\n\nCode\ncov.gaus &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         c0 + cs,\n         cs * exp(-(h/a)^2))\n}\n\ncov.mat &lt;- function(h, c0, cs, a, kappa = 1.35){\n  ifelse(h == 0,\n         c0 + cs,\n         cs * (1 - (1 - ((h / a)^kappa * besselK(h / a, nu = kappa)) /\n                 (2^(kappa - 1) * gamma(kappa)))))\n}\n\nloglik.model &lt;- function(par, Z, coords, cov.model) {\n  c0 &lt;- par[1]  \n  cs &lt;- par[2]   \n  a  &lt;- par[3]   \n  \n  \n  h &lt;- as.matrix(dist(coords))\n  \n  \n  Sigma &lt;- cov.model(h, c0, cs, a)\n  \n  \n  n &lt;- length(Z)\n  L &lt;- try(chol(Sigma), silent = TRUE)\n  if (inherits(L, \"try-error\")) return(1e6)\n  \n  \n  SinvZ &lt;- backsolve(L, forwardsolve(t(L), Z))\n  \n  logdet &lt;- 2 * sum(log(diag(L)))\n  nll &lt;- 0.5 * (n * log(2*pi) + logdet + sum(Z * SinvZ))\n  \n  return(nll)  \n}\n\n\nDe esta forma, se realiza el ajuste por máxima verosimilitud con la función optim.\n\n\nCode\ncoordinates(OZ) &lt;- ~ Easting + Northing\n\nmv.gaus &lt;- optim(\n  par = coef(nls.gaus1),\n  fn = loglik.model,\n  Z = OZ$OzC,\n  coords = coordinates(OZ),\n  cov.model = cov.gaus,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.gaus &lt;- within(mv.gaus, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_gaus(data_OZ$h, c0, cs, a), data_OZ$gamma_est))\n  gamma_pred &lt;- with(as.list(par), gamma_gaus(data_OZ$h, c0, cs, a))\n})\n\n\nmv.mat &lt;- optim(\n  par = init_mat,\n  fn = loglik.model,\n  Z = OZ$OzC,\n  coords = coordinates(OZ),\n  cov.model = cov.mat,\n  method = \"L-BFGS-B\",\n  lower = c(0, 0, 0)\n)\n\nmv.mat &lt;- within(mv.mat, {\n  MSE        &lt;- with(as.list(par), MSE(gamma_mat(data_OZ$h, c0, cs, a, kappa = 0.7), data_OZ$gamma_est))\n  gamma_pred &lt;- with(as.list(par), gamma_mat(data_OZ$h, c0, cs, a, kappa = 0.7))\n})\n\n\nY se presentan los resultados:\n\n\n\nComparación de estimaciones MV\n\n\nModelo\nMSE\n\n\n\n\nGausiano\n26.7394\n\n\nMatern\n127.5045"
  },
  {
    "objectID": "geostatistics/ozone.html#kriging-ordinario",
    "href": "geostatistics/ozone.html#kriging-ordinario",
    "title": "Análisis del Ozono",
    "section": "Kriging Ordinario",
    "text": "Kriging Ordinario\nBasado en los resultados pasados, vimos que el mejor modelo fue el Gausiano con la estimación de los parámetros a través de la función nls.\nPrimero, se genera la grilla de valores para poder realizar la interpolación.\n\ngrid &lt;- read.csv(\"grid.csv\", check.names = FALSE)\n\ncoordinates(grid) &lt;- ~ X + Y\n\nVeamos primero el modelo ajustado.\n\n\nCode\nbest_model &lt;- vgm(psill = 34.509, \n                  model = \"Gau\", \n                  range = 857720.5474, \n                  nugget = 9.8194)\n\n\nvgm &lt;- variogram(Ozone ~ 1, data = OZ, cutoff = max(dist(coordinates(OZ))))\n\nplot(vgm, best_model, \n     lwd = 2, col = \"dodgerblue2\",\n     main = \"Mejor modelo ajustado para la variable Ozono\")\n\n\n\n\n\n\n\n\n\nY se realiza el kriging ordinario basado en este modelo.\n\n\nCode\nkrige.oz &lt;- gstat::krige(\n  Ozone ~ 1,\n  locations = OZ,\n  newdata = grid,\n  model = best_model, maxdist = 1/3 * max(dist(coordinates(OZ))), nmax = 60)\n\n\n[using ordinary kriging]\n\n\n\nPredicciónError de Predicción\n\n\n\n\nCode\nStations_sp &lt;- as(coords_proj, \"Spatial\")\n\nstations_layer &lt;- list(\n  \"sp.points\",\n  Stations_sp,\n  pch = 21,      \n  cex = 1.6,\n  bg = viridis(100),\n  col = \"black\")\n\nrng    &lt;- range(krige.oz$var1.pred, na.rm = TRUE)\n\nat_lab &lt;- pretty(rng, n = 10)          \n\nspplot(krige.oz, \"var1.pred\", colorkey = list(\n        right = list(\n          fun = draw.colorkey,\n          args = list(\n            key = list(\n              at = at_lab,\n              col = viridis(100),\n              labels = list(\n                at = at_lab\n              )\n            )\n          )\n        )\n      ), \n      col.regions = viridis(100),\n      sp.layout   = list(stations_layer),\n      main = \"Mapa de Predicción del Ozono (ppb)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nstations_layer &lt;- list(\n  \"sp.points\",\n  Stations_sp,\n  pch = 21,      \n  cex = 1.6,\n  bg = viridis(100),\n  col = \"white\")\n\nrng    &lt;- range(krige.oz$var1.var, na.rm = TRUE)\n\nat_lab &lt;- pretty(rng, n = 10)  \n\nspplot(krige.oz, \"var1.var\", colorkey = list(\n        right = list(\n          fun = draw.colorkey,\n          args = list(\n            key = list(\n              at = at_lab,\n              col = magma(100),\n              labels = list(\n                at = at_lab\n              )\n            )\n          )\n        )\n      ), \n      col.regions = magma(100),\n      sp.layout   = list(stations_layer),\n      main = \"Error de Predicción del Ozono (ppb)\")\n\n\n\n\n\n\n\n\n\n\n\n\nEs posible ver que en hacia el occidente de Texas se presentan los valores más bajos de Ozono, mientras que en el oriente y, más especificamente hacia el suroriente, se presentan los valores más altos de ozono. Además, como es de esperarse, los errores de predicción más bajos se encuentran sobre la zona donde se tienen más estaciones, mientras que los más altos donde se tienen pocas."
  },
  {
    "objectID": "geostatistics/fd.html",
    "href": "geostatistics/fd.html",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Analizar las estructuras temporales y espaciales de las variables PM2.5 y ozono en el estado de Texas durante el año 2024 mediante técnicas de análisis de datos funcionales (FDA), con el fin de construir un modelo de covarianza espacial basado en componentes principales funcionales (FPCA) que permita realizar predicciones espacialmente coherentes de ambas variables.\n\n\n\n\n\nConstruir las curvas funcionales de PM2.5 y temperatura mediante un proceso de suavizamiento basado en bases B-splines cúbicas, estimando el parámetro de penalización λ mediante validación cruzada.\nAplicar el Análisis de Componentes Principales Funcionales (FPCA) para obtener los scores funcionales y estudiar la estructura de variabilidad dominante en las curvas diarias.\nModelar la dependencia espacial de los scores funcionales mediante la construcción del semivariograma empírico y el ajuste de modelos teóricos (Exponencial, Wave, Cúbico).\nImplementar Kriging Funcional para generar mapas de predicción espacial y de incertidumbre, utilizando los componentes principales y la estructura de covarianza estimada, con el propósito de representar la distribución esperada de PM2.5 y ozono y evaluar la precisión de las estimaciones.\n\n\n\n\nPrimero se cargan los datos y se muestra la estructura general que ellos tienen. En las filas se encuentra cada uno de los tiempos y en las columnas se encuentran las estaciones.\n\nOZ_st_pvt &lt;- read.csv(\"OZ_data_imputed.csv\",\n                      check.names = FALSE)\n\nOZ_st_pvt &lt;- OZ_st_pvt %&gt;%\n  mutate(across(where(is.numeric), ~ .x * 1000))\n\nhead(OZ_st_pvt, c(10, 5)) %&gt;% kable(format = \"pandoc\", escape = FALSE)\n\n\n\n\nDate\n480271045\n480271047\n480290032\n480290052\n\n\n\n\n01/01/2024\n26\n26.59725\n30\n29\n\n\n01/02/2024\n23\n19.40062\n15\n18\n\n\n01/03/2024\n23\n24.05877\n24\n25\n\n\n01/04/2024\n37\n33.97910\n25\n25\n\n\n01/05/2024\n32\n33.00000\n33\n35\n\n\n01/06/2024\n27\n30.00000\n34\n34\n\n\n01/07/2024\n38\n40.00000\n43\n43\n\n\n01/08/2024\n36\n38.00000\n36\n37\n\n\n01/09/2024\n36\n37.00000\n33\n31\n\n\n01/10/2024\n36\n37.04478\n35\n37\n\n\n\n\n\nDe esta forma, convertimos este objeto a una matriz y se convierte en un objeto funcional.\n\nOZ_num &lt;- OZ_st_pvt[,-1]\nOZ.matrix &lt;- as.matrix(OZ_num)\n\nOZ.fd &lt;- fdata(t(OZ.matrix), argvals = 1:nrow(OZ.matrix))\n\nplot(OZ.fd, main = \"Datos Funcionales de Ozono\", ylim = c(0, 120))\n\n\n\n\n\n\n\n\n\n\n\nSe eligen a los B-Splines como la base de funciones con la cual se va a trabajar. En la figura se puede apreciar las funciones y el tamaño de la base (\\(K=25\\)).\n\nnbasis &lt;- 25\n\ndayrange &lt;- c(1, nrow(OZ.matrix))\ndailybasis &lt;- create.fourier.basis(dayrange,period=366/365,nbasis)\nharmaccelLfd &lt;- vec2Lfd(c(1,76), dayrange)\n\nBSpl &lt;- create.bspline.basis(dayrange, nbasis)\nplot(BSpl)\n\n\n\n\n\n\n\n\nPor lo tanto, se obtienen las curvas con los datos de Ozono. De esta forma, se observa que son similares a los datos funcionales observados.\n\nOZ.fdPar_Bspline &lt;- fdPar(fdobj = BSpl, Lfdobj = harmaccelLfd)\n\nOZ.fd_Bspline &lt;- smooth.basis(argvals = 1:nrow(OZ.matrix), OZ.matrix, OZ.fdPar_Bspline)\nOZ.fd_Bspl &lt;- OZ.fd_Bspline$fd\n\nplot(OZ.fd_Bspl, col = \"white\", xlab = \"Día\", ylab = \"Ozono\")\n\n[1] \"done\"\n\nlines(OZ.fd_Bspl,col=rainbow(10),lwd=1.5,lty=1)\n\n\n\n\n\n\n\n\nPara hallar un buen valor del parámetro de suavizamiento \\(\\lambda\\), se realiza validación cruzada generalizada (GCV) de la siguiente forma.\n\nloglam &lt;- seq(-10, 5, by = 0.1)\nnlam   &lt;- length(loglam)\ngcvsave &lt;- numeric(nlam)\n\nfor (ilam in 1:nlam) {\n  lambda   &lt;- exp(loglam[ilam])\n  fdParobj &lt;- fdPar(BSpl, Lfdobj = harmaccelLfd, lambda = lambda)\n  \n  smoothlist &lt;- smooth.basis(argvals = 1:366, y = OZ.matrix, fdParobj)\n  \n  gcvsave[ilam] &lt;- mean(smoothlist$gcv)\n}\n\n\nplot(loglam, gcvsave, type = \"b\", cex = 0.7,\n     xlab = expression(log(lambda)),\n     ylab = expression(GCV(lambda)),\n     main = \"Parámetros de suavizamiento versus GCV\")\n\n\n\n\n\n\n\nbest.idx     &lt;- which.min(gcvsave)\nbest.loglam  &lt;- loglam[best.idx]\nbest.lambda  &lt;- exp(best.loglam)\n#best.lambda #0.001661557\n\nObteniendo que\n\n\n\n\n\n\\(\\lambda_{GCV}\\)\n\n\n\n\n0.0016616\n\n\n\n\n\nDe esta forma se vuelven a ajustar estas curvas, pero con este parámetro de suavizamiento. Obteniendo el siguiente resultado:\n\nOZ.fdPar_Bspline &lt;- fdPar(fdobj = BSpl, \n                            Lfdobj = harmaccelLfd, \n                            lambda = best.lambda)\n\nOZ.fd_Bspline &lt;- smooth.basis(argvals = 1:nrow(OZ.matrix),\n                                y = OZ.matrix, \n                                fdParobj = OZ.fdPar_Bspline)\n\nOZ.fd_Bspl &lt;- OZ.fd_Bspline$fd\n\nplot(OZ.fd_Bspl, col = rainbow(11), lwd = 1.5, lty = 1,\n     xlab = \"Tiempo\", ylab = \"Ozono\")\n\n\n\n\n\n\n\n\n[1] \"done\"\n\n\n\n\n\nAhora, se realiza un análisis de componentes principales. En este caso, se puede notar que únicamente con las dos primeras componentes se puede obtener un porcentaje de varianza explicado del 80% aproximadamente.\n\nOZ.fd_PCA &lt;- pca.fd(OZ.fd_Bspl, centerfns = TRUE, nharm = 10)\n\n\nDim &lt;- 1:10\n\nPerc &lt;- as.numeric(round(OZ.fd_PCA$varprop * 100, 2))\n\npar(mar = c(5, 4, 4, 2))\n\nbarplot(\n  Perc,\n  names.arg = Dim,\n  border = NA,\n  ylim = c(0, max(Perc) + 5),\n  main = \"Valores propios del FPCA\",\n  xlab = \"Dimensiones\",\n  ylab = \"Porcentaje de varianza explicada\"\n)\n\n\nbp &lt;- barplot(Perc, plot = FALSE)\n\n\nlines(bp, Perc, lwd = 2)\n\n\npoints(bp, Perc, pch = 16)\n\n\ntext(bp, Perc + 1, paste0(round(Perc, 1), \"%\"), cex = 0.8)\n\n\n\n\n\n\n\n\nPor lo tanto, guardamos estos scores con el fin de poder realizar kriging funcional de las curvas en lugares no muestreados.\n\nOZ.fd_PCA &lt;- pca.fd(OZ.fd_Bspl, centerfns = TRUE, nharm = 2)\n\n\nOZ_stations &lt;- read.csv(\"OZ_Stations.csv\", check.names = FALSE)\n\n\npuntaje &lt;- OZ.fd_PCA$scores\ncolnames(puntaje) &lt;- c(\"f1\",\"f2\")\nrownames(puntaje) &lt;- colnames(OZ_st_pvt)[colnames(OZ_st_pvt) != \"Date\"]\npuntajes &lt;- as.data.frame(puntaje) \ncoordinates(puntajes) &lt;- OZ_stations[,c(\"X\", \"Y\")]\n\npuntaje &lt;- data.frame(puntaje, puntajes@coords)\n\n\n\nAhora, analizamos la tendencia de las dos primeras componentes principales funcionales.\n\npar(mfrow = c(1,2))\nplot(puntajes$f1, puntajes@coords[,\"X\"],\n     xlab = \"f1\", ylab = \"X\")\nplot(puntajes$f1, puntajes@coords[,\"Y\"],\n     xlab = \"f1\", ylab = \"Y\")\n\n\n\n\n\n\n\npar(mfrow = c(1,2))\nplot(puntajes$f2, puntajes@coords[,\"X\"],\n     xlab = \"f2\", ylab = \"X\")\nplot(puntajes$f2, puntajes@coords[,\"Y\"],\n     xlab = \"f2\", ylab = \"Y\")\n\n\n\n\n\n\n\n\nComo se puede apreciar, los scores no presentan estacionariedad con respecto a la media. Por lo tanto, se intenta ajustar un modelo tal que ahora sí lo sean.\nPara el primer scores, el mejor modelo que se ajustó fue\n\\[\\text{score}_i^1 = \\beta_0 + \\beta_1x + \\beta_2y\\] Mientras que para el segundo, se obtuvo el modelo\n\\[\\text{score}_i^2 = \\beta_0 + \\beta_1x + \\beta_2y + \\beta_3y^2\\]\nDe esta forma,\n\nfit.f1 &lt;- lm(f1 ~ X + Y, data = puntaje)\n\npuntaje$f1res &lt;- resid(fit.f1)\n\npar(mfrow = c(1,2))\n\nplot(resid(fit.f1) ~ puntaje$X,\n     xlab = \"Easting\", ylab = \"Residuales\", main = \"Score 1\")\nplot(resid(fit.f1) ~ puntaje$Y,\n     xlab = \"Northing\", ylab = \"Residuales\", main = \"Score 1\")\n\n\n\n\n\n\n\n\n\nfit.f2 &lt;- lm(f2 ~ X + poly(Y, 2), data = puntaje)\n\npuntaje$f2res &lt;- resid(fit.f2)\n\npar(mfrow = c(1,2))\nplot(resid(fit.f2) ~ puntaje$X,\n     xlab = \"Easting\", ylab = \"Residuales\", main = \"Score 2\")\nplot(resid(fit.f2) ~ puntaje$Y,\n     xlab = \"Northing\", ylab = \"Residuales\", main = \"Score 2\")\n\n\n\n\n\n\n\n\nDe esta forma, se puede ver que ahora sí hay estacionariedad al trabajar con los residuales de los modelos ajustados para cada score. De esta forma, ahora es posible encontrar los modelos de semivarianza que mejor se ajusten a los semivariogramas empíricos de los scores.\n\n\n\nPrimero creamos el objeto gstat para luego obtener los semivariogramas de los scores y así, poder ajustar modelos teóricos.\n\nf1.vgm &lt;- variogram(f1 ~ X + Y, puntajes)\n\nf2.vgm &lt;- variogram(f2 ~ X + poly(Y, 2), puntajes)\n\n\nfd.v.cross &lt;- gstat(NULL, id = \"f1\", form = f1 ~ X + Y, data = puntajes)\nfd.v.cross &lt;- gstat(fd.v.cross, id = \"f2\", form = f2 ~ X + poly(Y, 2), data = puntajes)\n\nvgm.cross &lt;- variogram(fd.v.cross)\nplot(vgm.cross, xlab = \"Distancia\", ylab = \"Semivarianza\")\n\n\n\n\n\n\n\n\n\n\nPrimero, con ayuda de eyefit, obtenemos algunos modelos potencias y valores iniciales para proceder con la estimación de los parámetros.\n\n\nPara el primer score, se halló que los potenciales modelos son dos: Gausiano y Wave. Se presentan sus valores iniciales.\n\n\nCode\n# Gausiano\ninit_gau &lt;- c(c0 = 241.88, cs = 2338.16, a = 60034.02)\n \n# Wave\ninit_wav &lt;- c(c0 = 322.5, cs = 2015.65, a = 30017.01)\n\n\n\ngamma_gaus &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,\n         c0 + cs*(1 - exp(-(h/a)^2)))\n}\n\ngamma_wave &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,  \n         c0 + cs * (1 - (sin(h/a) / (h/a))))\n}\n\nMSE &lt;- function(fitted, observed){\n  \n  mean((fitted - observed)^2)\n  \n}\n\n\ndata &lt;- data.frame(h = f1.vgm$dist,\n                   gamma_hat = f1.vgm$gamma,\n                   n = f1.vgm$np)\n\nSe ajustan los modelos mediante las funciones nls y nlsLM. Se obtienen los siguientes resultados:\n\nnls.gaus1 &lt;- nls(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_gau),\n                 algorithm = \"port\",\n                 lower = c(0, 0, 0))\n\nctrl &lt;- nls.control(maxiter = 1000, tol = 1e-10, minFactor = 1/1024)\nnls.gaus2 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data,\n                   start = as.list(init_gau),\n                   weights = n,\n                   control = ctrl,\n                   lower = c(0, 0, 0))\n\nnls.gaus3 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data, \n                   start = as.list(init_gau),\n                   weights = (n/h^2),\n                   control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10),\n                   lower = c(0, 0, 0))\n\n\nnls.wave1 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav))\n\n\nnls.wave3 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 algorithm = \"port\",\n                 lower = c(0, 0, 0), \n                 weights = (n/h^2))\n\n\n\n\nMSE para cada modelo y peso\n\n\nModelo\nMSE\nPeso\n\n\n\n\nGausiano\n243089.9\n\\(1\\)\n\n\nGausiano\n248831.4\n\\(n\\)\n\n\nGausiano\n294734.9\n\\(\\frac{n}{h^2}\\)\n\n\nWave\n159914.7\n\\(1\\)\n\n\nWave\n227581.0\n\\(\\frac{n}{h^2}\\)\n\n\n\n\n\nDe esta forma, vemos que el que mejor ajustó fue el modelo Wave sin pesos. Observemos este modelo con los parámetros estimados.\n\n\n\n\n\n\n\n\n\nSe prueba el método de estimación Cressie con la matriz de pesos cambiando en cada iteración:\n\n\nCode\nQ_cressie &lt;- function(par, data, gamma_fun, kappa = NULL, eps = 1e-12){\n  \n  g &lt;- if (is.null(kappa)) {\n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"])\n    \n  } else {\n    \n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"], kappa = kappa)\n    \n  }\n  \n  g2 &lt;- pmax(g^2, eps)         \n  \n  sum( data$n * (data$gamma_hat - g)^2 / (2 * g2), na.rm = TRUE )\n}\n\n\nfit_cressie &lt;- function(gamma_fun, start, data,\n                        lower = c(c0=0, cs=0, a=.Machine$double.eps),\n                        upper = c(c0=Inf, cs=Inf, a=Inf),\n                        kappa = NULL){\n  \n  par0 &lt;- start[c(\"c0\",\"cs\",\"a\")]    \n  \n  opt &lt;- optim(par = par0, fn = Q_cressie, method = \"L-BFGS-B\",\n               lower = lower, upper = upper,\n               data = data, gamma_fun = gamma_fun, kappa = kappa)\n  \n  \n  p  &lt;- opt$par\n  \n  g  &lt;- if (is.null(kappa)) {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"])\n  } else {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"], kappa)\n  }    \n  \n  mse &lt;- MSE(g, data$gamma_hat)\n  \n  list(par = p, converged = (opt$convergence==0),\n       fitted = g, MSE = mse, kappa = kappa)\n}\n\n\nY se realiza el ajuste de los modelos.\n\ncress.gaus &lt;- fit_cressie(gamma_gaus, start = init_gau, data = data)\n\ncress.wave &lt;- fit_cressie(gamma_wave, start = init_wav, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE para ajustes Cressie\n\n\nModelo\nMSE\n\n\n\n\nGausiano\n245898.6\n\n\nWave\n170948.1\n\n\n\n\n\nDe esta forma, se observa que se siguen teniendo mejores resultados con el método de regresión no lineal.\n\n\n\nDe nuevo, con ayuda de eyefit se encuentra que los modelos con una mayor similitud al semivariograma empírico son el Gausiano y el Wave. Se inicializan los parámetros en:\n\n\nCode\n# Gausiano\ninit_gau &lt;- c(c0 = 148.02, cs = 1040.17, a = 46693.13)\n\n# Wave\ninit_wav &lt;- c(c0 = 148.02, cs = 1040.17, a = 31128.75)\n\n\nSe ajustan los modelos mediante regresión no lineal.\n\ndata &lt;- data.frame(h = f2.vgm$dist,\n                   gamma_hat = f2.vgm$gamma,\n                   n = f2.vgm$np)\n\nnls.gaus1 &lt;- nls(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_gau),\n                 algorithm = \"port\",\n                 lower = c(0, 0, 0))\n\nctrl &lt;- nls.control(maxiter = 1000, tol = 1e-10, minFactor = 1/1024)\nnls.gaus2 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data,\n                   start = as.list(init_gau),\n                   weights = n,\n                   control = ctrl,\n                   lower = c(0, 0, 0))\n\nnls.gaus3 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data, \n                   start = as.list(init_gau),\n                   weights = (n/h^2),\n                   control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10),\n                   lower = c(0, 0, 0))\n\n\nnls.wave1 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav))\n\nnls.wave2 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 weights = 1/n,\n                 algorithm = \"port\",\n                 lower = c(0, 0, 0))\n\n\nnls.wave3 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 algorithm = \"port\",\n                 lower = c(0, 0, 0), \n                 weights = (n/h^2))\n\nObteniendo los siguientes resultados:\n\n\n\nMSE para cada modelo y peso\n\n\nModelo\nMSE\nPeso\n\n\n\n\nGaussiano\n87114.72\n\\(1\\)\n\n\nGaussiano\n87367.20\n\\(n\\)\n\n\nGaussiano\n154793.73\n\\(\\frac{n}{h^2}\\)\n\n\nWave\n68123.90\n\\(1\\)\n\n\nWave\n68553.19\n\\(n\\)\n\n\nWave\n118020.07\n\\(\\frac{n}{h^2}\\)\n\n\n\n\n\nEncontrando una vez más que, el mejor modelo corresponde al Wave sin considerar pesos en la estimación.\n\nplot(f2.vgm$dist, f2.vgm$gamma,\n     pch = 19, xlab = \"Distancia\", ylab = \"Semivarianza\",\n     main = \"Ajuste con el mejor modelo\")\nlines(x = data$h, y = fitted(nls.wave1),\n      col = \"dodgerblue\", lwd = 3) # el mejor de todos\n\n\n\n\n\n\n\nf2.model &lt;- list(par = coef(nls.wave1),\n                 model = \"Wav\")\n\nDe esta forma, ya se tiene el ajuste de los modelos teróricos para cada score.\nSe prueba además, con otro método de estimación.\n\ncress.gaus &lt;- fit_cressie(gamma_gaus, start = init_gau, data = data)\n\ncress.wave &lt;- fit_cressie(gamma_wave, start = init_wav, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE para ajustes Cressie\n\n\nModelo\nMSE\n\n\n\n\nGausiano\n97228.47\n\n\nWave\n73001.86\n\n\n\n\n\nObteniendo un mejor ajuste por el método de Cressie apra el modelo Wave.\n\n\n\n\n\n\nYa obtenidos los modelos se modifica el objeto gstat que se había definido anteriormente basado en las modelos ajustados.\n\n\nCode\nfd.v.cross &lt;- gstat(fd.v.cross, id = \"f1\", \n                    model = vgm(1691.1, \"Wav\", 3*28992.4, 502.2), \n                    fill.all = FALSE)\n\nfd.v.cross &lt;- gstat(fd.v.cross, id = \"f2\", \n                    model = vgm(831.2681, \"Wav\", 3*26174.5293, 180.7270), \n                    fill.all = FALSE)\n\n\nDe esta forma, ahora definimos un nuevo objeto SpatFD y se usa la función KS_scores_lambdas. Luego, con reconstruimos las curvas, obteniendo las predicciones en los lugares no muestreados:\n\n\nCode\ngrid &lt;- read.csv(\"grid.csv\", check.names = FALSE)\n\n\nspat_fd &lt;- SpatFD(data = OZ.matrix, coords = OZ_stations[,c(\"X\", \"Y\")],\n                  basis = \"Bsplines\", nbasis = nbasis,\n                  lambda = best.lambda, nharm = 2)\n\nKS_sco &lt;- KS_scores_lambdas(spat_fd, newcoords = grid, model = fd.v.cross$model,\n                            method = \"scores\")\n\n\nUsing first variable by default\n\n\nUsing fill.all = TRUE by default\n\n\nCode\ncurves_sco &lt;- recons_fd(KS_sco)\n\nplot(curves_sco, main = \"Kriging Funcional Ozono (Scores)\", \n     xlab = \"Días\", ylab = \"Ozono\")\n\n\n\n\n\n\n\n\n\nSe puede observar que las curvas reconstruidas mediante el kriging por el método de scores en los lugares no muestreados conservan los comportamientos temporales de las estaciones reales.\nPara obtener las sucesiones de superficies espaciales, se consideran cuatro fechas en específico, espaciadas temporalmente quince días.\n\neval &lt;- fda::eval.fd(1:366, curves_sco) # Matriz Predicciones\n\ntiempos &lt;- c(15, 30, 45, 60)\n\n# 1) Obtener rango global de todos los valores\nvalores &lt;- unlist(eval[tiempos, ])   # valores de los tiempos\nrng_global &lt;- range(valores, na.rm = TRUE)\nat_lab &lt;- pretty(rng_global, n = 10)\n\n\n\nplots.fd &lt;- list()\nk &lt;- 1\n\npar(mfrow=c(2,2))\nfor (t in c(15, 30, 45, 60)) {\n  fd.pred &lt;- data.frame(coordinates(grid), Ozone = eval[t,]) # Cada fecha\n  coordinates(fd.pred) &lt;- ~ X + Y\n  \n   \n  \n  p.fd &lt;- spplot(fd.pred, \"Ozone\", colorkey = list(\n                  right = list(\n                    fun = draw.colorkey,\n                    args = list(\n                      key = list(\n                        at = at_lab,\n                        col = viridis(100),\n                        labels = list(at = at_lab)\n                      )\n                    )\n                  )\n                ), \n                col.regions = viridis(100),\n                main = paste0(\"Predicción (scores) en t=\",t))\n  \n  plots.fd[[k]] &lt;- p.fd\n  k &lt;- k + 1\n}\n\n\ngridExtra::grid.arrange(grobs = plots.fd, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\nDe estas gráficas es posible ver que los patrones espaciales son similares a través del tiempo, teniendo temperaturas más altas hacia el noroccidente y el sur de Texas, mientras que los valores más bajos de temperatura se pueden observar hacie el oriente."
  },
  {
    "objectID": "geostatistics/fd.html#objetivo-general",
    "href": "geostatistics/fd.html#objetivo-general",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Analizar las estructuras temporales y espaciales de las variables PM2.5 y ozono en el estado de Texas durante el año 2024 mediante técnicas de análisis de datos funcionales (FDA), con el fin de construir un modelo de covarianza espacial basado en componentes principales funcionales (FPCA) que permita realizar predicciones espacialmente coherentes de ambas variables."
  },
  {
    "objectID": "geostatistics/fd.html#objetivos-específicos",
    "href": "geostatistics/fd.html#objetivos-específicos",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Construir las curvas funcionales de PM2.5 y temperatura mediante un proceso de suavizamiento basado en bases B-splines cúbicas, estimando el parámetro de penalización λ mediante validación cruzada.\nAplicar el Análisis de Componentes Principales Funcionales (FPCA) para obtener los scores funcionales y estudiar la estructura de variabilidad dominante en las curvas diarias.\nModelar la dependencia espacial de los scores funcionales mediante la construcción del semivariograma empírico y el ajuste de modelos teóricos (Exponencial, Wave, Cúbico).\nImplementar Kriging Funcional para generar mapas de predicción espacial y de incertidumbre, utilizando los componentes principales y la estructura de covarianza estimada, con el propósito de representar la distribución esperada de PM2.5 y ozono y evaluar la precisión de las estimaciones."
  },
  {
    "objectID": "geostatistics/fd.html#descripción",
    "href": "geostatistics/fd.html#descripción",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Primero se cargan los datos y se muestra la estructura general que ellos tienen. En las filas se encuentra cada uno de los tiempos y en las columnas se encuentran las estaciones.\n\nOZ_st_pvt &lt;- read.csv(\"OZ_data_imputed.csv\",\n                      check.names = FALSE)\n\nOZ_st_pvt &lt;- OZ_st_pvt %&gt;%\n  mutate(across(where(is.numeric), ~ .x * 1000))\n\nhead(OZ_st_pvt, c(10, 5)) %&gt;% kable(format = \"pandoc\", escape = FALSE)\n\n\n\n\nDate\n480271045\n480271047\n480290032\n480290052\n\n\n\n\n01/01/2024\n26\n26.59725\n30\n29\n\n\n01/02/2024\n23\n19.40062\n15\n18\n\n\n01/03/2024\n23\n24.05877\n24\n25\n\n\n01/04/2024\n37\n33.97910\n25\n25\n\n\n01/05/2024\n32\n33.00000\n33\n35\n\n\n01/06/2024\n27\n30.00000\n34\n34\n\n\n01/07/2024\n38\n40.00000\n43\n43\n\n\n01/08/2024\n36\n38.00000\n36\n37\n\n\n01/09/2024\n36\n37.00000\n33\n31\n\n\n01/10/2024\n36\n37.04478\n35\n37\n\n\n\n\n\nDe esta forma, convertimos este objeto a una matriz y se convierte en un objeto funcional.\n\nOZ_num &lt;- OZ_st_pvt[,-1]\nOZ.matrix &lt;- as.matrix(OZ_num)\n\nOZ.fd &lt;- fdata(t(OZ.matrix), argvals = 1:nrow(OZ.matrix))\n\nplot(OZ.fd, main = \"Datos Funcionales de Ozono\", ylim = c(0, 120))"
  },
  {
    "objectID": "geostatistics/fd.html#b-splines",
    "href": "geostatistics/fd.html#b-splines",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Se eligen a los B-Splines como la base de funciones con la cual se va a trabajar. En la figura se puede apreciar las funciones y el tamaño de la base (\\(K=25\\)).\n\nnbasis &lt;- 25\n\ndayrange &lt;- c(1, nrow(OZ.matrix))\ndailybasis &lt;- create.fourier.basis(dayrange,period=366/365,nbasis)\nharmaccelLfd &lt;- vec2Lfd(c(1,76), dayrange)\n\nBSpl &lt;- create.bspline.basis(dayrange, nbasis)\nplot(BSpl)\n\n\n\n\n\n\n\n\nPor lo tanto, se obtienen las curvas con los datos de Ozono. De esta forma, se observa que son similares a los datos funcionales observados.\n\nOZ.fdPar_Bspline &lt;- fdPar(fdobj = BSpl, Lfdobj = harmaccelLfd)\n\nOZ.fd_Bspline &lt;- smooth.basis(argvals = 1:nrow(OZ.matrix), OZ.matrix, OZ.fdPar_Bspline)\nOZ.fd_Bspl &lt;- OZ.fd_Bspline$fd\n\nplot(OZ.fd_Bspl, col = \"white\", xlab = \"Día\", ylab = \"Ozono\")\n\n[1] \"done\"\n\nlines(OZ.fd_Bspl,col=rainbow(10),lwd=1.5,lty=1)\n\n\n\n\n\n\n\n\nPara hallar un buen valor del parámetro de suavizamiento \\(\\lambda\\), se realiza validación cruzada generalizada (GCV) de la siguiente forma.\n\nloglam &lt;- seq(-10, 5, by = 0.1)\nnlam   &lt;- length(loglam)\ngcvsave &lt;- numeric(nlam)\n\nfor (ilam in 1:nlam) {\n  lambda   &lt;- exp(loglam[ilam])\n  fdParobj &lt;- fdPar(BSpl, Lfdobj = harmaccelLfd, lambda = lambda)\n  \n  smoothlist &lt;- smooth.basis(argvals = 1:366, y = OZ.matrix, fdParobj)\n  \n  gcvsave[ilam] &lt;- mean(smoothlist$gcv)\n}\n\n\nplot(loglam, gcvsave, type = \"b\", cex = 0.7,\n     xlab = expression(log(lambda)),\n     ylab = expression(GCV(lambda)),\n     main = \"Parámetros de suavizamiento versus GCV\")\n\n\n\n\n\n\n\nbest.idx     &lt;- which.min(gcvsave)\nbest.loglam  &lt;- loglam[best.idx]\nbest.lambda  &lt;- exp(best.loglam)\n#best.lambda #0.001661557\n\nObteniendo que\n\n\n\n\n\n\\(\\lambda_{GCV}\\)\n\n\n\n\n0.0016616\n\n\n\n\n\nDe esta forma se vuelven a ajustar estas curvas, pero con este parámetro de suavizamiento. Obteniendo el siguiente resultado:\n\nOZ.fdPar_Bspline &lt;- fdPar(fdobj = BSpl, \n                            Lfdobj = harmaccelLfd, \n                            lambda = best.lambda)\n\nOZ.fd_Bspline &lt;- smooth.basis(argvals = 1:nrow(OZ.matrix),\n                                y = OZ.matrix, \n                                fdParobj = OZ.fdPar_Bspline)\n\nOZ.fd_Bspl &lt;- OZ.fd_Bspline$fd\n\nplot(OZ.fd_Bspl, col = rainbow(11), lwd = 1.5, lty = 1,\n     xlab = \"Tiempo\", ylab = \"Ozono\")\n\n\n\n\n\n\n\n\n[1] \"done\""
  },
  {
    "objectID": "geostatistics/fd.html#pca",
    "href": "geostatistics/fd.html#pca",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Ahora, se realiza un análisis de componentes principales. En este caso, se puede notar que únicamente con las dos primeras componentes se puede obtener un porcentaje de varianza explicado del 80% aproximadamente.\n\nOZ.fd_PCA &lt;- pca.fd(OZ.fd_Bspl, centerfns = TRUE, nharm = 10)\n\n\nDim &lt;- 1:10\n\nPerc &lt;- as.numeric(round(OZ.fd_PCA$varprop * 100, 2))\n\npar(mar = c(5, 4, 4, 2))\n\nbarplot(\n  Perc,\n  names.arg = Dim,\n  border = NA,\n  ylim = c(0, max(Perc) + 5),\n  main = \"Valores propios del FPCA\",\n  xlab = \"Dimensiones\",\n  ylab = \"Porcentaje de varianza explicada\"\n)\n\n\nbp &lt;- barplot(Perc, plot = FALSE)\n\n\nlines(bp, Perc, lwd = 2)\n\n\npoints(bp, Perc, pch = 16)\n\n\ntext(bp, Perc + 1, paste0(round(Perc, 1), \"%\"), cex = 0.8)\n\n\n\n\n\n\n\n\nPor lo tanto, guardamos estos scores con el fin de poder realizar kriging funcional de las curvas en lugares no muestreados.\n\nOZ.fd_PCA &lt;- pca.fd(OZ.fd_Bspl, centerfns = TRUE, nharm = 2)\n\n\nOZ_stations &lt;- read.csv(\"OZ_Stations.csv\", check.names = FALSE)\n\n\npuntaje &lt;- OZ.fd_PCA$scores\ncolnames(puntaje) &lt;- c(\"f1\",\"f2\")\nrownames(puntaje) &lt;- colnames(OZ_st_pvt)[colnames(OZ_st_pvt) != \"Date\"]\npuntajes &lt;- as.data.frame(puntaje) \ncoordinates(puntajes) &lt;- OZ_stations[,c(\"X\", \"Y\")]\n\npuntaje &lt;- data.frame(puntaje, puntajes@coords)\n\n\n\nAhora, analizamos la tendencia de las dos primeras componentes principales funcionales.\n\npar(mfrow = c(1,2))\nplot(puntajes$f1, puntajes@coords[,\"X\"],\n     xlab = \"f1\", ylab = \"X\")\nplot(puntajes$f1, puntajes@coords[,\"Y\"],\n     xlab = \"f1\", ylab = \"Y\")\n\n\n\n\n\n\n\npar(mfrow = c(1,2))\nplot(puntajes$f2, puntajes@coords[,\"X\"],\n     xlab = \"f2\", ylab = \"X\")\nplot(puntajes$f2, puntajes@coords[,\"Y\"],\n     xlab = \"f2\", ylab = \"Y\")\n\n\n\n\n\n\n\n\nComo se puede apreciar, los scores no presentan estacionariedad con respecto a la media. Por lo tanto, se intenta ajustar un modelo tal que ahora sí lo sean.\nPara el primer scores, el mejor modelo que se ajustó fue\n\\[\\text{score}_i^1 = \\beta_0 + \\beta_1x + \\beta_2y\\] Mientras que para el segundo, se obtuvo el modelo\n\\[\\text{score}_i^2 = \\beta_0 + \\beta_1x + \\beta_2y + \\beta_3y^2\\]\nDe esta forma,\n\nfit.f1 &lt;- lm(f1 ~ X + Y, data = puntaje)\n\npuntaje$f1res &lt;- resid(fit.f1)\n\npar(mfrow = c(1,2))\n\nplot(resid(fit.f1) ~ puntaje$X,\n     xlab = \"Easting\", ylab = \"Residuales\", main = \"Score 1\")\nplot(resid(fit.f1) ~ puntaje$Y,\n     xlab = \"Northing\", ylab = \"Residuales\", main = \"Score 1\")\n\n\n\n\n\n\n\n\n\nfit.f2 &lt;- lm(f2 ~ X + poly(Y, 2), data = puntaje)\n\npuntaje$f2res &lt;- resid(fit.f2)\n\npar(mfrow = c(1,2))\nplot(resid(fit.f2) ~ puntaje$X,\n     xlab = \"Easting\", ylab = \"Residuales\", main = \"Score 2\")\nplot(resid(fit.f2) ~ puntaje$Y,\n     xlab = \"Northing\", ylab = \"Residuales\", main = \"Score 2\")\n\n\n\n\n\n\n\n\nDe esta forma, se puede ver que ahora sí hay estacionariedad al trabajar con los residuales de los modelos ajustados para cada score. De esta forma, ahora es posible encontrar los modelos de semivarianza que mejor se ajusten a los semivariogramas empíricos de los scores.\n\n\n\nPrimero creamos el objeto gstat para luego obtener los semivariogramas de los scores y así, poder ajustar modelos teóricos.\n\nf1.vgm &lt;- variogram(f1 ~ X + Y, puntajes)\n\nf2.vgm &lt;- variogram(f2 ~ X + poly(Y, 2), puntajes)\n\n\nfd.v.cross &lt;- gstat(NULL, id = \"f1\", form = f1 ~ X + Y, data = puntajes)\nfd.v.cross &lt;- gstat(fd.v.cross, id = \"f2\", form = f2 ~ X + poly(Y, 2), data = puntajes)\n\nvgm.cross &lt;- variogram(fd.v.cross)\nplot(vgm.cross, xlab = \"Distancia\", ylab = \"Semivarianza\")\n\n\n\n\n\n\n\n\n\n\nPrimero, con ayuda de eyefit, obtenemos algunos modelos potencias y valores iniciales para proceder con la estimación de los parámetros.\n\n\nPara el primer score, se halló que los potenciales modelos son dos: Gausiano y Wave. Se presentan sus valores iniciales.\n\n\nCode\n# Gausiano\ninit_gau &lt;- c(c0 = 241.88, cs = 2338.16, a = 60034.02)\n \n# Wave\ninit_wav &lt;- c(c0 = 322.5, cs = 2015.65, a = 30017.01)\n\n\n\ngamma_gaus &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,\n         c0 + cs*(1 - exp(-(h/a)^2)))\n}\n\ngamma_wave &lt;- function(h, c0, cs, a){\n  ifelse(h == 0,\n         0,  \n         c0 + cs * (1 - (sin(h/a) / (h/a))))\n}\n\nMSE &lt;- function(fitted, observed){\n  \n  mean((fitted - observed)^2)\n  \n}\n\n\ndata &lt;- data.frame(h = f1.vgm$dist,\n                   gamma_hat = f1.vgm$gamma,\n                   n = f1.vgm$np)\n\nSe ajustan los modelos mediante las funciones nls y nlsLM. Se obtienen los siguientes resultados:\n\nnls.gaus1 &lt;- nls(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_gau),\n                 algorithm = \"port\",\n                 lower = c(0, 0, 0))\n\nctrl &lt;- nls.control(maxiter = 1000, tol = 1e-10, minFactor = 1/1024)\nnls.gaus2 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data,\n                   start = as.list(init_gau),\n                   weights = n,\n                   control = ctrl,\n                   lower = c(0, 0, 0))\n\nnls.gaus3 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data, \n                   start = as.list(init_gau),\n                   weights = (n/h^2),\n                   control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10),\n                   lower = c(0, 0, 0))\n\n\nnls.wave1 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav))\n\n\nnls.wave3 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 algorithm = \"port\",\n                 lower = c(0, 0, 0), \n                 weights = (n/h^2))\n\n\n\n\nMSE para cada modelo y peso\n\n\nModelo\nMSE\nPeso\n\n\n\n\nGausiano\n243089.9\n\\(1\\)\n\n\nGausiano\n248831.4\n\\(n\\)\n\n\nGausiano\n294734.9\n\\(\\frac{n}{h^2}\\)\n\n\nWave\n159914.7\n\\(1\\)\n\n\nWave\n227581.0\n\\(\\frac{n}{h^2}\\)\n\n\n\n\n\nDe esta forma, vemos que el que mejor ajustó fue el modelo Wave sin pesos. Observemos este modelo con los parámetros estimados.\n\n\n\n\n\n\n\n\n\nSe prueba el método de estimación Cressie con la matriz de pesos cambiando en cada iteración:\n\n\nCode\nQ_cressie &lt;- function(par, data, gamma_fun, kappa = NULL, eps = 1e-12){\n  \n  g &lt;- if (is.null(kappa)) {\n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"])\n    \n  } else {\n    \n    gamma_fun(h = data$h, c0 = par[\"c0\"], cs = par[\"cs\"], a = par[\"a\"], kappa = kappa)\n    \n  }\n  \n  g2 &lt;- pmax(g^2, eps)         \n  \n  sum( data$n * (data$gamma_hat - g)^2 / (2 * g2), na.rm = TRUE )\n}\n\n\nfit_cressie &lt;- function(gamma_fun, start, data,\n                        lower = c(c0=0, cs=0, a=.Machine$double.eps),\n                        upper = c(c0=Inf, cs=Inf, a=Inf),\n                        kappa = NULL){\n  \n  par0 &lt;- start[c(\"c0\",\"cs\",\"a\")]    \n  \n  opt &lt;- optim(par = par0, fn = Q_cressie, method = \"L-BFGS-B\",\n               lower = lower, upper = upper,\n               data = data, gamma_fun = gamma_fun, kappa = kappa)\n  \n  \n  p  &lt;- opt$par\n  \n  g  &lt;- if (is.null(kappa)) {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"])\n  } else {\n    gamma_fun(data$h, p[\"c0\"], p[\"cs\"], p[\"a\"], kappa)\n  }    \n  \n  mse &lt;- MSE(g, data$gamma_hat)\n  \n  list(par = p, converged = (opt$convergence==0),\n       fitted = g, MSE = mse, kappa = kappa)\n}\n\n\nY se realiza el ajuste de los modelos.\n\ncress.gaus &lt;- fit_cressie(gamma_gaus, start = init_gau, data = data)\n\ncress.wave &lt;- fit_cressie(gamma_wave, start = init_wav, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE para ajustes Cressie\n\n\nModelo\nMSE\n\n\n\n\nGausiano\n245898.6\n\n\nWave\n170948.1\n\n\n\n\n\nDe esta forma, se observa que se siguen teniendo mejores resultados con el método de regresión no lineal.\n\n\n\nDe nuevo, con ayuda de eyefit se encuentra que los modelos con una mayor similitud al semivariograma empírico son el Gausiano y el Wave. Se inicializan los parámetros en:\n\n\nCode\n# Gausiano\ninit_gau &lt;- c(c0 = 148.02, cs = 1040.17, a = 46693.13)\n\n# Wave\ninit_wav &lt;- c(c0 = 148.02, cs = 1040.17, a = 31128.75)\n\n\nSe ajustan los modelos mediante regresión no lineal.\n\ndata &lt;- data.frame(h = f2.vgm$dist,\n                   gamma_hat = f2.vgm$gamma,\n                   n = f2.vgm$np)\n\nnls.gaus1 &lt;- nls(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_gau),\n                 algorithm = \"port\",\n                 lower = c(0, 0, 0))\n\nctrl &lt;- nls.control(maxiter = 1000, tol = 1e-10, minFactor = 1/1024)\nnls.gaus2 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data,\n                   start = as.list(init_gau),\n                   weights = n,\n                   control = ctrl,\n                   lower = c(0, 0, 0))\n\nnls.gaus3 &lt;- nlsLM(gamma_hat ~ gamma_gaus(h, c0, cs, a),\n                   data = data, \n                   start = as.list(init_gau),\n                   weights = (n/h^2),\n                   control = nls.lm.control(maxiter=1000, ftol=1e-10, ptol=1e-10),\n                   lower = c(0, 0, 0))\n\n\nnls.wave1 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav))\n\nnls.wave2 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 weights = 1/n,\n                 algorithm = \"port\",\n                 lower = c(0, 0, 0))\n\n\nnls.wave3 &lt;- nls(gamma_hat ~ gamma_wave(h, c0, cs, a),\n                 data = data,\n                 start = as.list(init_wav), \n                 algorithm = \"port\",\n                 lower = c(0, 0, 0), \n                 weights = (n/h^2))\n\nObteniendo los siguientes resultados:\n\n\n\nMSE para cada modelo y peso\n\n\nModelo\nMSE\nPeso\n\n\n\n\nGaussiano\n87114.72\n\\(1\\)\n\n\nGaussiano\n87367.20\n\\(n\\)\n\n\nGaussiano\n154793.73\n\\(\\frac{n}{h^2}\\)\n\n\nWave\n68123.90\n\\(1\\)\n\n\nWave\n68553.19\n\\(n\\)\n\n\nWave\n118020.07\n\\(\\frac{n}{h^2}\\)\n\n\n\n\n\nEncontrando una vez más que, el mejor modelo corresponde al Wave sin considerar pesos en la estimación.\n\nplot(f2.vgm$dist, f2.vgm$gamma,\n     pch = 19, xlab = \"Distancia\", ylab = \"Semivarianza\",\n     main = \"Ajuste con el mejor modelo\")\nlines(x = data$h, y = fitted(nls.wave1),\n      col = \"dodgerblue\", lwd = 3) # el mejor de todos\n\n\n\n\n\n\n\nf2.model &lt;- list(par = coef(nls.wave1),\n                 model = \"Wav\")\n\nDe esta forma, ya se tiene el ajuste de los modelos teróricos para cada score.\nSe prueba además, con otro método de estimación.\n\ncress.gaus &lt;- fit_cressie(gamma_gaus, start = init_gau, data = data)\n\ncress.wave &lt;- fit_cressie(gamma_wave, start = init_wav, data = data)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMSE para ajustes Cressie\n\n\nModelo\nMSE\n\n\n\n\nGausiano\n97228.47\n\n\nWave\n73001.86\n\n\n\n\n\nObteniendo un mejor ajuste por el método de Cressie apra el modelo Wave."
  },
  {
    "objectID": "geostatistics/fd.html#kriging-funcional-scores",
    "href": "geostatistics/fd.html#kriging-funcional-scores",
    "title": "Análisis de Datos Funcionales",
    "section": "",
    "text": "Ya obtenidos los modelos se modifica el objeto gstat que se había definido anteriormente basado en las modelos ajustados.\n\n\nCode\nfd.v.cross &lt;- gstat(fd.v.cross, id = \"f1\", \n                    model = vgm(1691.1, \"Wav\", 3*28992.4, 502.2), \n                    fill.all = FALSE)\n\nfd.v.cross &lt;- gstat(fd.v.cross, id = \"f2\", \n                    model = vgm(831.2681, \"Wav\", 3*26174.5293, 180.7270), \n                    fill.all = FALSE)\n\n\nDe esta forma, ahora definimos un nuevo objeto SpatFD y se usa la función KS_scores_lambdas. Luego, con reconstruimos las curvas, obteniendo las predicciones en los lugares no muestreados:\n\n\nCode\ngrid &lt;- read.csv(\"grid.csv\", check.names = FALSE)\n\n\nspat_fd &lt;- SpatFD(data = OZ.matrix, coords = OZ_stations[,c(\"X\", \"Y\")],\n                  basis = \"Bsplines\", nbasis = nbasis,\n                  lambda = best.lambda, nharm = 2)\n\nKS_sco &lt;- KS_scores_lambdas(spat_fd, newcoords = grid, model = fd.v.cross$model,\n                            method = \"scores\")\n\n\nUsing first variable by default\n\n\nUsing fill.all = TRUE by default\n\n\nCode\ncurves_sco &lt;- recons_fd(KS_sco)\n\nplot(curves_sco, main = \"Kriging Funcional Ozono (Scores)\", \n     xlab = \"Días\", ylab = \"Ozono\")\n\n\n\n\n\n\n\n\n\nSe puede observar que las curvas reconstruidas mediante el kriging por el método de scores en los lugares no muestreados conservan los comportamientos temporales de las estaciones reales.\nPara obtener las sucesiones de superficies espaciales, se consideran cuatro fechas en específico, espaciadas temporalmente quince días.\n\neval &lt;- fda::eval.fd(1:366, curves_sco) # Matriz Predicciones\n\ntiempos &lt;- c(15, 30, 45, 60)\n\n# 1) Obtener rango global de todos los valores\nvalores &lt;- unlist(eval[tiempos, ])   # valores de los tiempos\nrng_global &lt;- range(valores, na.rm = TRUE)\nat_lab &lt;- pretty(rng_global, n = 10)\n\n\n\nplots.fd &lt;- list()\nk &lt;- 1\n\npar(mfrow=c(2,2))\nfor (t in c(15, 30, 45, 60)) {\n  fd.pred &lt;- data.frame(coordinates(grid), Ozone = eval[t,]) # Cada fecha\n  coordinates(fd.pred) &lt;- ~ X + Y\n  \n   \n  \n  p.fd &lt;- spplot(fd.pred, \"Ozone\", colorkey = list(\n                  right = list(\n                    fun = draw.colorkey,\n                    args = list(\n                      key = list(\n                        at = at_lab,\n                        col = viridis(100),\n                        labels = list(at = at_lab)\n                      )\n                    )\n                  )\n                ), \n                col.regions = viridis(100),\n                main = paste0(\"Predicción (scores) en t=\",t))\n  \n  plots.fd[[k]] &lt;- p.fd\n  k &lt;- k + 1\n}\n\n\ngridExtra::grid.arrange(grobs = plots.fd, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\nDe estas gráficas es posible ver que los patrones espaciales son similares a través del tiempo, teniendo temperaturas más altas hacia el noroccidente y el sur de Texas, mientras que los valores más bajos de temperatura se pueden observar hacie el oriente."
  },
  {
    "objectID": "geostatistics/spacetime.html",
    "href": "geostatistics/spacetime.html",
    "title": "Espacio Tiempo",
    "section": "",
    "text": "En este apartado se trabajará con los 8 días anteiores al 15 de abril de 2024."
  },
  {
    "objectID": "geostatistics/spacetime.html#análisis-descriptivo",
    "href": "geostatistics/spacetime.html#análisis-descriptivo",
    "title": "Espacio Tiempo",
    "section": "Análisis Descriptivo",
    "text": "Análisis Descriptivo\nPrimero realizamos la carga de los datos y las estaciones. Los datos se encuentran organizados de la siguiente forma:\n\nStations &lt;- read.csv(\"temp_stations_NA.csv\", check.names = FALSE)\nStations &lt;- Stations %&gt;% mutate(AQSID = as.character(AQSID))\n\nTemp_st_pvt &lt;- read.csv(\"Temp_data_imputed.csv\", check.names = FALSE)\n\nTemp_st &lt;- Temp_st_pvt %&gt;% \n  pivot_longer(\n    cols = -Valid.Date,\n    names_to = \"AQSID\",\n    values_to = \"Temp\"\n  ) %&gt;% \n  mutate(Valid.Date = as.Date(Valid.Date, format = \"%Y-%m-%d\"),\n         Day = yday(Valid.Date), \n         AQSID = as.character(AQSID)) \n\nTemp.st &lt;- Temp_st %&gt;% \n  filter(Day %in% seq(90, 97, 1)) %&gt;% \n  left_join(Stations, by = c(\"AQSID\")) %&gt;% \n  dplyr::select(Valid.Date, AQSID, Temp, Day, X, Y) \n\n\ndataTemp.ST &lt;- Temp.st %&gt;% \n  arrange(AQSID, Valid.Date) %&gt;%\n  dplyr::select(Temp)\n\n\ntime.index &lt;- sort(unique(Temp.st$Valid.Date))\n\nTemp.stations.sp &lt;- Stations %&gt;% arrange(AQSID)\n\nTemp.stations.sp &lt;- SpatialPoints(Temp.stations.sp[,c(\"X\", \"Y\")])\n\n\nTEMP.ST &lt;- STFDF(Temp.stations.sp, time.index, dataTemp.ST)\n\n\n\n\nDatos Espacio Temporales de Temperatura\n\n\nValid.Date\nAQSID\nTemp\nDay\nX\nY\n\n\n\n\n2024-03-30\n480271045\n24.0\n90\n1744479\n7447518\n\n\n2024-03-30\n480271047\n23.5\n90\n1720906\n7443160\n\n\n2024-03-30\n480290032\n24.1\n90\n1633570\n7266958\n\n\n2024-03-30\n480290052\n23.1\n90\n1638725\n7280038\n\n\n2024-03-30\n480290055\n23.7\n90\n1652022\n7255235\n\n\n2024-03-30\n480290059\n24.2\n90\n1663815\n7240764\n\n\n2024-03-30\n480290501\n24.8\n90\n1623771\n7240383\n\n\n2024-03-30\n480290502\n22.2\n90\n1632732\n7290822\n\n\n2024-03-30\n480290622\n24.6\n90\n1661634\n7249348\n\n\n2024-03-30\n480290677\n24.1\n90\n1637522\n7256897\n\n\n\n\n\nY, para estos ocho días, se tienen las siguientes observaciones.\n\n\nCode\nstplot(TEMP.ST, main = \"Temperatura en ocho días consecutivos\")\n\n\n\n\n\n\n\n\n\n\nEstacionariedad\nSe analiza la estacionariedad de la media en estas ocho estaciones. Inicialmente, la temperatura presentaba tendencia espacial, por lo que se ajustó un modelo general que pudiese servir para todos los tiempos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe esta forma se puede presenciar una estacionariedad general en los distintos tiempos que se están analizando.\nAdemás, se presentan el mapa de calor y las series por estación. En el primero, podemos ver que en el eje X se presentan algunas de las estaciones, el eje Y representa las fechas consideradas y el color indicaría la temperatura. En el segundo gráfico podemos ver que las series no presentan tendencia marcada, indicando además un comportamiento similar de la temperatura en estas veinte estaciones.\n\nPrimera GráficaSegunda Gráfica\n\n\n\nIDs &lt;- unique(Stations$AQSID)\nsel &lt;- NULL; for(i in 1:20) sel&lt;-c(sel,which(IDs==IDs[i]))\n\nstplot(TEMP.ST[sel, \"2024-03-31::2024-04-14\", \"Temp\"], mode=\"xt\", scaleX=0,\n       col.regions=terrain.colors(100), main = \"Temperatura\")\n\n\n\n\n\n\n\n\n\n\n\nstplot(TEMP.ST[sel, \"2024-03-31::2024-04-14\", \"Temp\"], mode=\"ts\",\n       main = \"Temperatura\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemivariograma\nA continuación se presenta el semivariograma empírico obtenido con la librería GeoModels.\n\ncoordinates(Temp.st) &lt;- ~ X + Y\n\nmaxdist &lt;- max(dist(coordinates(Temp.st)))\n\nTemp.st.res &lt;- as.data.frame(Temp.st) %&gt;%\n  group_by(Day) %&gt;% \n  mutate(Temp = resid(lm(Temp ~ X + Y + I(X^2)))) %&gt;%\n  ungroup() # se trabaja con los residuales\n\ngeoMatrix &lt;- Temp.st.res %&gt;% # los datos se necesitan como matriz\n  arrange(Day, AQSID) %&gt;%\n  select(Day, AQSID, Temp) %&gt;%\n  tidyr::pivot_wider(names_from = AQSID, values_from = Temp) %&gt;%\n  arrange(Day) %&gt;%\n  select(-Day) %&gt;%\n  as.matrix()\n\n\ngeovgm &lt;- GeoVariogram(data = geoMatrix, \n                       coordx = Stations[,c(\"X\", \"Y\")],\n                       coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                       distance = \"Eucl\", \n                       type= \"variogram\", \n                       maxdist = 1/3*maxdist)\nplot(geovgm)\n\n\n\n\n\n\n\n\nSe puede ver que, en el semivariograma temporal existe una estructura de dependencia. De igual forma, espacialmente se puede presenciar un nugget pequeño y la posible silla alrededor de 2.\n\nEstimación Teórica del Semivariograma\nPara llevar a cabo la estimación, se usó la función GeoFit de GeoModels. En la ayuda de esta función se especifican distintos modelos espacio tiempo separables y no separables. Se realizó el ajuste de varios de ellos, tomando como valores iniciales de escala los que se podían visualizar de los semivariogramas marginales.\n\nModelos No Separables\nA pesar de que se ajustaron siete modelos no separables, el algoritmo de todos ellos no convergió muy seguramente. Solo convergieron de forma segura los modelos Gneiting y Mátern Mátern.\nA continuación se presenta la estimación.\n\n\nCode\nmean_est &lt;- 0; var_est &lt;- var(as.numeric(geoMatrix))\n\n##### Gneiting ----\n\nCorrParam(\"Gneiting\")\n\ninit.ST_gne &lt;- list(power_s = 2,\n                    power_t = 0.8,\n                    scale_s = 2e5,\n                    scale_t = 3,\n                    sep = 0.5,\n                    sill = var_est,\n                    nugget = 0.1)\n\nfixed.ST_gne &lt;- list(mean = 0)\n\n\nGeoCovmatrix(corrmodel = \"Gneiting\", \n             coordx = Stations[,c(\"X\", \"Y\")],\n             coordt = 1:5,\n             model = \"Gaussian\",\n             param = init.ST_gne)\n\n\n\nlwr &lt;- list(power_s = 1e-4,\n            power_t = 1e-4,\n            scale_s = 0,\n            scale_t = 0,\n            sep = 0,\n            sill = 1/2,\n            nugget = 0)\n\nupr &lt;- list(power_s = 2,\n            power_t = 2,\n            scale_s = Inf,\n            scale_t = Inf,\n            sep = 1,\n            sill = Inf,\n            nugget = 1)\n\n\n\ngeofit_gne &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Gneiting\", \n                     start = init.ST_gne, \n                     fixed = fixed.ST_gne,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr,\n                     upper = upr)\n\ngeofit_gne$logCompLik #-1376261\n\ngeocgvm_gne &lt;- GeoCovariogram(fitted = geofit_gne, \n                              answer.vario = TRUE,\n                              show.vario = TRUE, \n                              add.vario = TRUE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\n\n##### Iacocesare ----\n\nCorrParam(\"Iacocesare\")\n\ninit.ST_iac &lt;- list(power2 = 3,\n                    power_s = 2,\n                    power_t = 0.8, \n                    scale_s = 2e5,\n                    scale_t = 3,\n                    sill = var_est,\n                    nugget = 0.1)\n\nfixed.ST_iac &lt;- list(mean = 0)\n\n\nlwr.Iac &lt;- list(power2 = 1e-4,\n                power_s = 1e-4,\n                power_t = 1e-4, \n                scale_s = 0,\n                scale_t = 0,\n                sill = 1/2,\n                nugget = 0.1)\n\nupr.Iac &lt;- list(power2 = 2,\n                power_s = 2,\n                power_t = 2, \n                scale_s = Inf,\n                scale_t = Inf,\n                sill = Inf,\n                nugget = 1)\n\ngeofit_iac &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Iacocesare\", \n                     start = init.ST_iac, \n                     fixed = fixed.ST_iac,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr.Iac,\n                     upper = upr.Iac)\n\ngeofit_iac$logCompLik # -1376174\n\ngeocgvm_Iac &lt;- GeoCovariogram(fitted = geofit_iac, \n                              answer.vario = TRUE,\n                              show.vario = TRUE, \n                              add.vario = FALSE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\n\n\n##### Porcu ----\n\nCorrParam(\"Porcu\")\n\ninit.ST_Por &lt;- list(power_s = 2,\n                    power_t = 0.8, \n                    scale_s = 2e5,\n                    scale_t = 3,\n                    sill = var_est,\n                    nugget = 0.1,\n                    sep = 0.5)\n\nfixed.ST_Por &lt;- list(mean = 0)\n\n\nlwr.Por &lt;- list(power_s = 1e-4,\n                power_t = 1e-4, \n                scale_s = 0,\n                scale_t = 0,\n                sill = 1/2,\n                nugget = 0.1,\n                sep = 0)\n\nupr.Por &lt;- list(power_s = 2,\n                power_t = 2, \n                scale_s = Inf,\n                scale_t = Inf,\n                sill = Inf,\n                nugget = 1,\n                sep = 1)\n\ngeofit_Por &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Porcu\", \n                     start = init.ST_Por, \n                     fixed = fixed.ST_Por,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr.Por,\n                     upper = upr.Por)\n\ngeofit_Por$logCompLik # -1376660\n\ngeocgvm_Por &lt;- GeoCovariogram(fitted = geofit_Por, \n                              answer.vario = TRUE,\n                              show.vario = TRUE, \n                              add.vario = TRUE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\n\n###### Porcu 1 ----\n\n\nCorrParam(\"Porcu\")\n\ninit.ST_Por1 &lt;- list(power_s = 2,\n                     power_t = 0.8, \n                     scale_s = 2e5,\n                     scale_t = 3,\n                     sill = var_est,\n                     nugget = 0.1,\n                     sep = 0.5)\n\nfixed.ST_Por1 &lt;- list(mean = 0)\n\n\nlwr.Por1 &lt;- list(power_s = 1e-4,\n                 power_t = 1e-4, \n                 scale_s = 0,\n                 scale_t = 0,\n                 sill = 1/2,\n                 nugget = 0.1,\n                 sep = 0)\n\nupr.Por1 &lt;- list(power_s = 2,\n                 power_t = 2, \n                 scale_s = Inf,\n                 scale_t = Inf,\n                 sill = Inf,\n                 nugget = 1,\n                 sep = 1)\n\ngeofit_Por1 &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Porcu1\", \n                     start = init.ST_Por1, \n                     fixed = fixed.ST_Por1,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr.Por1,\n                     upper = upr.Por1)\n\ngeofit_Por1$logCompLik # -1374162\n\ngeocgvm_Por1 &lt;- GeoCovariogram(fitted = geofit_Por1, \n                              answer.vario = TRUE,\n                              show.vario = TRUE, \n                              add.vario = TRUE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\n\n##### Gneiting_mat_S ----\n\nCorrParam(\"Gneiting_mat_S\")\n\ninit.ST_GMS &lt;- list(power_t = 0.8, \n                    power2_t = 2,\n                    scale_s = 2e5,\n                    scale_t = 3,\n                    smooth_s = 2,\n                    sill = var_est,\n                    nugget = 0.1,\n                    sep = 0.5)\n\nfixed.ST_GMS &lt;- list(mean = 0)\n\nGeoCovmatrix(corrmodel = \"Gneiting_mat_S\", \n             coordx = Stations[,c(\"X\", \"Y\")],\n             coordt = 1:5,\n             model = \"Gaussian\",\n             param = init.ST_GMS)\n\n\nlwr.GMS &lt;- list(power_t = 1e-4, \n                power2_t = 1e-4,\n                scale_s = 0,\n                scale_t = 0,\n                smooth_s = 1e-4,\n                sill = 0,\n                nugget = 1e-4,\n                sep = 0)\n\nupr.GMS &lt;- list(power_t = 2, \n                power2_t = 2,\n                scale_s = Inf,\n                scale_t = Inf,\n                smooth_s = Inf,\n                sill = Inf,\n                nugget = 2,\n                sep = 1)\n\ngeofit_GMS &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Gneiting_mat_S\", \n                     start = init.ST_GMS, \n                     fixed = fixed.ST_GMS,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr.GMS,\n                     upper = upr.GMS)\n\ngeofit_GMS$logCompLik # -1376660\n\ngeocgvm_GMS &lt;- GeoCovariogram(fitted = geofit_GMS, \n                              answer.vario = TRUE,\n                              show.vario = TRUE, \n                              add.vario = TRUE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\n\n\n##### Gneiting_mat_T ----\n\nCorrParam(\"Gneiting_mat_T\")\n\ninit.ST_GMT &lt;- list(power_t = 0.8, \n                    power2_t = 2,\n                    scale_s = 2e5,\n                    scale_t = 3,\n                    smooth_s = 2,\n                    sill = var_est,\n                    nugget = 0.1,\n                    sep = 0.5)\n\nfixed.ST_GMT &lt;- list(mean = 0)\n\nGeoCovmatrix(corrmodel = \"Gneiting_mat_S\", \n             coordx = Stations[,c(\"X\", \"Y\")],\n             coordt = 1:5,\n             model = \"Gaussian\",\n             param = init.ST_GMT)\n\n\nlwr.GMT &lt;- list(power_t = 1e-4, \n                power2_t = 1e-4,\n                scale_s = 0,\n                scale_t = 0,\n                smooth_s = 1e-4,\n                sill = 0,\n                nugget = 1e-4,\n                sep = 0)\n\nupr.GMT &lt;- list(power_t = 2, \n                power2_t = 2,\n                scale_s = Inf,\n                scale_t = Inf,\n                smooth_s = Inf,\n                sill = Inf,\n                nugget = 2,\n                sep = 1)\n\ngeofit_GMT &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Gneiting_mat_S\", \n                     start = init.ST_GMT, \n                     fixed = fixed.ST_GMT,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr.GMT,\n                     upper = upr.GMT)\n\ngeofit_GMT$logCompLik # -1376660\n\ngeocgvm_GMT &lt;- GeoCovariogram(fitted = geofit_GMT, \n                              answer.vario = TRUE,\n                              show.vario = TRUE, \n                              add.vario = TRUE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\n\n\n##### Matern_Matern_nosep ----\n\n\nCorrParam(\"Matern_Matern_nosep\")\n\ninit.ST_Mat_Mat &lt;- list(scale_s = 2e5,\n                        scale_t = 3,\n                        smooth_s = 1,\n                        smooth_t = 1,\n                        sill = var_est,\n                        nugget = 0.1,\n                        sep = 0.5)\n\nfixed.ST &lt;- list(mean = 0)\n\n\nGeoCovmatrix(corrmodel = \"Matern_Matern_nosep\", \n             coordx = Stations[,c(\"X\", \"Y\")],\n             coordt = 1:5,\n             model = \"Gaussian\",\n             param = init.ST_Mat_Mat)\n\n\n\nlwr.Mat_Mat &lt;- list(scale_s = 0,\n                    scale_t = 0,\n                    smooth_s = 1e-10,\n                    smooth_t = 1e-10,\n                    sill = 1/2,\n                    nugget = 0.01,\n                    sep = 0)\n\nupr.Mat_Mat &lt;- list(scale_s = Inf,\n                    scale_t = Inf,\n                    smooth_s = Inf,\n                    smooth_t = Inf,\n                    sill = Inf,\n                    nugget = 2,\n                    sep = 1)\n\ngeofit_Mat_Mat &lt;- GeoFit(data = geoMatrix, \n                         coordx = Stations[,c(\"X\", \"Y\")],\n                         coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                         distance = \"Eucl\", \n                         maxtime = 4, maxdist = maxdist/3,\n                         model = \"Gaussian\", \n                         corrmodel = \"Matern_Matern_nosep\", \n                         start = init.ST_Mat_Mat, \n                         fixed = fixed.ST,\n                         optimizer = \"L-BFGS-B\",\n                         lower = lwr.Mat_Mat,\n                         upper = upr.Mat_Mat)\n\ngeofit_Mat_Mat$logCompLik #-1375451 \n\ngeocgvm_Mat_Mat &lt;- GeoCovariogram(fitted = geofit_Mat_Mat, \n                                  answer.vario = TRUE,\n                                  show.vario = TRUE, \n                                  add.vario = TRUE,\n                                  vario = geovgm, \n                                  invisible = TRUE)\n\n\n\n##### Multiquadric_st ----\n\n\nCorrParam(\"Multiquadric_st\")\n\ninit.ST_Mul &lt;- list(power_s = 2,\n                    power_t = 0.8, \n                    scale_s = 2e5,\n                    scale_t = 3,\n                    sill = var_est,\n                    nugget = 0.1)\n\nfixed.ST_Mul &lt;- list(mean = 0)\n\n\nlwr.Mul &lt;- list(power_s = 1e-4,\n                power_t = 1e-4, \n                scale_s = 0,\n                scale_t = 0,\n                sill = 1/2,\n                nugget = 0.001)\n\nupr.Mul &lt;- list(power_s = 2,\n                power_t = 2, \n                scale_s = Inf,\n                scale_t = Inf,\n                sill = Inf,\n                nugget = 2)\n\ngeofit_Mul &lt;- GeoFit(data = geoMatrix, \n                     coordx = Stations[,c(\"X\", \"Y\")],\n                     coordt = unique(Temp.st$Day - min(Temp.st$Day)),\n                     distance = \"Eucl\", \n                     maxtime = 4, maxdist = maxdist/3,\n                     model = \"Gaussian\", \n                     corrmodel = \"Multiquadric_st\", \n                     start = init.ST_Mul, \n                     fixed = fixed.ST_Mul,\n                     optimizer = \"L-BFGS-B\",\n                     lower = lwr.Mul,\n                     upper = upr.Mul)\n\ngeofit_Mul$logCompLik # -1376779\n\ngeocgvm_Mul &lt;- GeoCovariogram(fitted = geofit_Mul,\n                              show.vario = TRUE,\n                              vario = geovgm, \n                              invisible = TRUE)\n\n\nPor motivos de eficiencia, se guardaron los resultados de estos modelos en un archivo aparte. Primero se muestra el semivariograma teórico obtenido con el modelo Mátern Mátern\n\nload(\"geofitST.RData\")\n\nGeoCovariogram(fitted = geofitST$MatMat, \n               answer.vario = TRUE,\n               show.vario = TRUE, \n               add.vario = TRUE,\n               vario = geovgm, \n               invisible = TRUE)\n\n\n\n\n\n\n\n\nY también del modelo Gneiting:\n\nGeoCovariogram(fitted = geofitST$Gneiting, \n               answer.vario = TRUE,\n               show.vario = TRUE, \n               add.vario = TRUE,\n               vario = geovgm, \n               invisible = TRUE)\n\n\n\n\n\n\n\n\nY se resumen sus log–verosimilitudes compuestas:\n\n\n\nComparación Modelos Espacio Tiempo\n\n\n\nLog-Verosimilitud Compuesta\n\n\n\n\nGneiting\n-1376261\n\n\nMatMat\n-1375451\n\n\n\n\n\nA pesar de que con el modelo Mátern Mátern obtenemos una log–verosimilitud mayor, al intentar realizar kriging espacio temporal, este da resultados que no tienen mucho sentido. Por lo tanto, se procede al análisis con el modelo Gneiting.\n\n\n\n\nKriging Espacio Tiempo\nSe va a realizar kriging espacio tiempo con el modelo Gneiting. Los parámetros obtenidos fueron los siguientes:\n\nbest_pars &lt;- data.frame(unlist(geofitST$Gneiting$param))\n\nbest_pars %&gt;% \n  kable(format = \"pandoc\",\n        digits = 4,\n        align = 'c',\n        escape = FALSE,\n        caption = \"Parámetros del modelo escogido\",\n        col.names = \"Estimación\")\n\n\nParámetros del modelo escogido\n\n\n\nEstimación\n\n\n\n\nnugget\n7.1340e-01\n\n\npower_s\n2.0000e+00\n\n\npower_t\n2.0000e+00\n\n\nscale_s\n2.0000e+05\n\n\nscale_t\n7.8320e-01\n\n\nsep\n0.0000e+00\n\n\nsill\n1.4624e+00\n\n\n\n\n\nSe carga el grid para poder realizar la predicción. Además, se considerarán tres horizontes.\n\ngrid &lt;- read.csv(\"grid.csv\", check.names = FALSE)\n\nloc_to_pred &lt;- as.matrix(as.data.frame(grid))\n\ntimes_to_pred &lt;- 9:11\n\nSe realiza el kriging espacio tiempo con ayuda de la función GeoKrige. De igual forma, por eficiencia, se guardaron los resultados en un archivo y se importan para su análisis.\n\n\nCode\nST.Krige &lt;- GeoKrig(estobj = geofitST$Gneiting,\n                    loc = loc_to_pred,\n                    time = times_to_pred,\n                    mse = TRUE)\n\n\nSe muestran los resultados a continuación.\n\npreds_ST &lt;- read.csv(\"kriging.ST.csv\", check.names = FALSE)\n\nTemp.st.or &lt;- as.data.frame(Temp.st) %&gt;% \n     arrange(Day, AQSID) %&gt;%\n     select(Day, AQSID, Temp) %&gt;%\n     tidyr::pivot_wider(names_from = AQSID, values_from = Temp) %&gt;%\n     arrange(Day) %&gt;%\n     select(-Day) %&gt;%\n     as.matrix()\n\nt1 &lt;- as.data.frame(Temp.st) %&gt;%\n    dplyr::filter(Day == 90) %&gt;%\n    lm(Temp ~ X + Y + I(X^2), data = .)\n\nmu.temp &lt;- function(coefs, X, Y){\n  \n  beta0 &lt;- coefs[1]\n  beta1 &lt;- coefs[2]\n  beta2 &lt;- coefs[3]\n  beta3 &lt;- coefs[4]\n  \n  beta0 + beta1 * X + beta2 * Y + beta3 * X^2\n}\n\n\npreds.mu &lt;- mu.temp(coef(t1), loc_to_pred[,1], loc_to_pred[,2])\n\ngrid_ST &lt;- grid\n\ngrid_ST$t1 &lt;- preds_ST$t1 + preds.mu\ngrid_ST$t2 &lt;- preds_ST$t2 + preds.mu\ngrid_ST$t3 &lt;- preds_ST$t3 + preds.mu\n\ngrid_ST &lt;- cbind(grid_ST, preds_ST[,c(\"mse1\", \"mse2\", \"mse3\")])\n\n\npred_vars &lt;- paste0(\"t\", 1:3)\nmse_vars  &lt;- paste0(\"mse\",  1:3)\n\n\ncoordinates(grid_ST) &lt;- ~ X+ Y\n\n\nplot_list &lt;- list()  # para guardar las figuras \n\nfor (i in 1:3) {\n  \n  pred_var &lt;- pred_vars[i]   \n  mse_var  &lt;- mse_vars[i]   \n  \n  # Predicción \n  rng_pred &lt;- range(grid_ST[[pred_var]], na.rm = TRUE)\n  at_pred  &lt;- pretty(rng_pred, n = 8)\n  \n  p_pred &lt;- spplot(\n    grid_ST, pred_var,\n    colorkey = list(\n      right = list(\n        fun  = draw.colorkey,\n        args = list(\n          key = list(\n            at   = at_pred,\n            col  = magma(100),\n            labels = list(at = at_pred)\n          )\n        )\n      )\n    ),\n    col.regions = magma(100),\n    main = paste(\"Predicción Temperatura t =\", i)\n  )\n  \n  ## --- MSE ---\n  rng_mse &lt;- range(grid_ST[[mse_var]], na.rm = TRUE)\n  at_mse  &lt;- pretty(rng_mse, n = 8)\n  \n  p_mse &lt;- spplot(\n    grid_ST, mse_var,\n    colorkey = list(\n      right = list(\n        fun  = draw.colorkey,\n        args = list(\n          key = list(\n            at   = at_mse,\n            col  = inferno(100),\n            labels = list(at = at_mse)\n          )\n        )\n      )\n    ),\n    col.regions = inferno(100),\n    main = paste(\"Error de Predicción t =\", i)\n  )\n  \n  \n  gridExtra::grid.arrange(p_pred, p_mse, ncol = 2)\n  \n  \n  plot_list[[paste0(\"t\", i)]] &lt;- list(pred = p_pred, mse = p_mse)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimero, podemos ver que en los lugares donde hay estaciones, el error es menor, lo cual es de esperar. Por otro lado, las predicciones parecen no cambiar mucho en estos tres días."
  },
  {
    "objectID": "geostatistics/index.html",
    "href": "geostatistics/index.html",
    "title": "Geostatistics",
    "section": "",
    "text": "Welcome to the geostatistics section. Here you will find the analysis of temperature and ozone in Texas, US, using different approaches such as spatial, spatio-temporal and functional kriging."
  },
  {
    "objectID": "geostatistics/index.html#topics",
    "href": "geostatistics/index.html#topics",
    "title": "Geostatistics",
    "section": "Topics",
    "text": "Topics\n\nTemperature\nSpatio-temporal models\nFunctional data"
  },
  {
    "objectID": "index.html#website-structure",
    "href": "index.html#website-structure",
    "title": "spatial-statistics",
    "section": "Website structure",
    "text": "Website structure\nThe content of this website is organized into the following sections:\n\nGeostatistics\nAreal Data"
  },
  {
    "objectID": "arealdata/datos_area.html",
    "href": "arealdata/datos_area.html",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "",
    "text": "El conjunto de datos utilizado corresponde al proyecto International Aid to Nepal (2007–14), desarrollado por el Center for Spatial Data Science (GeoDa Center). Su objetivo es integrar información geoespacial, socioeconómica y financiera que permita estudiar la pobreza, el desarrollo humano y la distribución de la ayuda internacional en los 75 distritos administrativos de Nepal. Este dataset combina datos espaciales y estadísticos.\n\n\n\n\n\n\n\n\n\nCaracterística\nDetalle\n\n\n\n\nTítulo\nInternational Aid to Nepal (2007–14)\n\n\nObservaciones\n75 distritos (un polígono por distrito)\n\n\nVariables\n61 variables\n\n\nCobertura temporal\n1991–2013 (ayuda internacional: 2007–2014)\n\n\nFuentes principales\nAidData y Open Nepal\n\n\nTipo de dato espacial\nPolígonos distritales (MULTIPOLYGON)\n\n\nCRS\nWGS84 – EPSG:4326\n\n\nArchivos shapefile\n.shp, .shx, .dbf, .prj, .qpj\n\n\n\n\n\n\nLas 61 variables se organizan en cinco grupos temáticos principales.\n\n\n\n\n\nVariable\nDescripción\nUnidad/Tipo\n\n\n\n\nid\nIdentificador único del distrito\nNumérica\n\n\nname_1\nRegión administrativa\nTexto\n\n\nname_2\nZona administrativa\nTexto\n\n\ndistrict\nNombre del distrito\nTexto\n\n\nlon\nLongitud del centroide\nGrados\n\n\nlat\nLatitud del centroide\nGrados\n\n\n\n\n\n\n\n\n\nVariable\nDescripción\nUnidad\n\n\n\n\nDepEcProv\nPrivación económica\nÍndice\n\n\nPovIndex\nÍndice de Pobreza Humana\nÍndice\n\n\nPCInc\nIngreso per cápita\nRs.\n\n\nPCIncPPP\nIngreso per cápita ajustado por PPA\nUSD PPP\n\n\nPCIncMP\nIngreso per cápita a precio de mercado\nRs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescripción\nUnidad\n\n\n\n\nPopulation\nPoblación total\nPersonas\n\n\nMalKids\n% de niños malnutridos (&lt; 5 años)\nPorcentaje\n\n\nLif40\n% que no alcanzará los 40 años\nPorcentaje\n\n\nNoSafH20\n% sin acceso a agua potable segura\nPorcentaje\n\n\nVotNum\nNúmero de votantes registrados\nPersonas\n\n\n\n\n\n\n\n\n\nVariable\nDescripción\nUnidad\n\n\n\n\nBoyG1_5\nNiños matriculados grado 1–5\nNúmero\n\n\nGirlG1_5\nNiñas matriculadas grado 1–5\nNúmero\n\n\nKIDS1_5\nTotal niños grado 1–5\nNúmero\n\n\nSchoolCnt\nNúmero de escuelas\nNúmero\n\n\nSCHLPKID\nEscuelas por 1.000 niños\nRatio\n\n\nSCHLPPOP\nEscuelas por 1.000 habitantes\nRatio\n\n\nAD_ILLIT\nAnalfabetismo adulto (2011)\nPorcentaje\n\n\nAD_ILGT50\n1 si analfabetismo &gt; 50%, 0 en otro caso\nBinaria\n\n\n\n\n\n\nCada sector contiene dos variables: xxCAMT (monto comprometido) y xxDAMT (monto desembolsado).\n\n\n\n\n\n\n\n\n\nCódigo\nSector\nCódigo\nSector\n\n\n\n\nAG\nAgricultura\nHEALT\nSalud\n\n\nBANK\nBanca\nHUM\nAyuda humanitaria\n\n\nCOMM\nComunicaciones\nIND\nIndustria\n\n\nCON\nConflicto\nMUL\nMultisectorial\n\n\nEDU\nEducación\nSOC\nInfraestructura social\n\n\nENGY\nEnergía\nTOUR\nTurismo\n\n\nENV\nMedio ambiente\nTRAN\nTransporte y almacenamiento\n\n\nFOR\nSilvicultura\nWAT\nAgua y saneamiento\n\n\nGOV\nGobierno y sociedad civil\nTOT\nTotal general"
  },
  {
    "objectID": "arealdata/datos_area.html#resumen-general-del-dataset",
    "href": "arealdata/datos_area.html#resumen-general-del-dataset",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "",
    "text": "Característica\nDetalle\n\n\n\n\nTítulo\nInternational Aid to Nepal (2007–14)\n\n\nObservaciones\n75 distritos (un polígono por distrito)\n\n\nVariables\n61 variables\n\n\nCobertura temporal\n1991–2013 (ayuda internacional: 2007–2014)\n\n\nFuentes principales\nAidData y Open Nepal\n\n\nTipo de dato espacial\nPolígonos distritales (MULTIPOLYGON)\n\n\nCRS\nWGS84 – EPSG:4326\n\n\nArchivos shapefile\n.shp, .shx, .dbf, .prj, .qpj"
  },
  {
    "objectID": "arealdata/datos_area.html#categorías-y-descripción-de-variables",
    "href": "arealdata/datos_area.html#categorías-y-descripción-de-variables",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "",
    "text": "Las 61 variables se organizan en cinco grupos temáticos principales.\n\n\n\n\n\nVariable\nDescripción\nUnidad/Tipo\n\n\n\n\nid\nIdentificador único del distrito\nNumérica\n\n\nname_1\nRegión administrativa\nTexto\n\n\nname_2\nZona administrativa\nTexto\n\n\ndistrict\nNombre del distrito\nTexto\n\n\nlon\nLongitud del centroide\nGrados\n\n\nlat\nLatitud del centroide\nGrados\n\n\n\n\n\n\n\n\n\nVariable\nDescripción\nUnidad\n\n\n\n\nDepEcProv\nPrivación económica\nÍndice\n\n\nPovIndex\nÍndice de Pobreza Humana\nÍndice\n\n\nPCInc\nIngreso per cápita\nRs.\n\n\nPCIncPPP\nIngreso per cápita ajustado por PPA\nUSD PPP\n\n\nPCIncMP\nIngreso per cápita a precio de mercado\nRs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescripción\nUnidad\n\n\n\n\nPopulation\nPoblación total\nPersonas\n\n\nMalKids\n% de niños malnutridos (&lt; 5 años)\nPorcentaje\n\n\nLif40\n% que no alcanzará los 40 años\nPorcentaje\n\n\nNoSafH20\n% sin acceso a agua potable segura\nPorcentaje\n\n\nVotNum\nNúmero de votantes registrados\nPersonas\n\n\n\n\n\n\n\n\n\nVariable\nDescripción\nUnidad\n\n\n\n\nBoyG1_5\nNiños matriculados grado 1–5\nNúmero\n\n\nGirlG1_5\nNiñas matriculadas grado 1–5\nNúmero\n\n\nKIDS1_5\nTotal niños grado 1–5\nNúmero\n\n\nSchoolCnt\nNúmero de escuelas\nNúmero\n\n\nSCHLPKID\nEscuelas por 1.000 niños\nRatio\n\n\nSCHLPPOP\nEscuelas por 1.000 habitantes\nRatio\n\n\nAD_ILLIT\nAnalfabetismo adulto (2011)\nPorcentaje\n\n\nAD_ILGT50\n1 si analfabetismo &gt; 50%, 0 en otro caso\nBinaria\n\n\n\n\n\n\nCada sector contiene dos variables: xxCAMT (monto comprometido) y xxDAMT (monto desembolsado).\n\n\n\n\n\n\n\n\n\nCódigo\nSector\nCódigo\nSector\n\n\n\n\nAG\nAgricultura\nHEALT\nSalud\n\n\nBANK\nBanca\nHUM\nAyuda humanitaria\n\n\nCOMM\nComunicaciones\nIND\nIndustria\n\n\nCON\nConflicto\nMUL\nMultisectorial\n\n\nEDU\nEducación\nSOC\nInfraestructura social\n\n\nENGY\nEnergía\nTOUR\nTurismo\n\n\nENV\nMedio ambiente\nTRAN\nTransporte y almacenamiento\n\n\nFOR\nSilvicultura\nWAT\nAgua y saneamiento\n\n\nGOV\nGobierno y sociedad civil\nTOT\nTotal general"
  },
  {
    "objectID": "arealdata/datos_area.html#pesos-espaciales",
    "href": "arealdata/datos_area.html#pesos-espaciales",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Pesos Espaciales",
    "text": "Pesos Espaciales\nDe esta forma, se procede con la definición de los vecinos para evaluar cuál es la matriz que mejor describe la relación espacial presente en los datos de analfabetismo de Nepal. En el caso de los vecinos físicos se obtienen las mismas conexiones para los métodos queen y rook.\n\n\nCode\n## Vecinos Físicos\n\nnepal.nb &lt;- poly2nb(nepal.utm, queen = TRUE)\nnepal.lw_w &lt;- nb2listw(nepal.nb, style = \"W\", zero.policy = TRUE)\nnepal.lw_b &lt;- nb2listw(nepal.nb, style = \"B\", zero.policy = TRUE)\n\n## Vecinos basados en grafos\n\ntri.nb &lt;- tri2nb(coords)\ntri.nb_w &lt;- nb2listw(tri.nb, style = \"W\", zero.policy = TRUE)\ntri.nb_b &lt;- nb2listw(tri.nb, style = \"B\", zero.policy = TRUE)\n\n\nsoi.nb &lt;- graph2nb(soi.graph(tri.nb, coords))\nsoi.nb_w &lt;- nb2listw(soi.nb, style = \"W\", zero.policy = TRUE)\nsoi.nb_b &lt;- nb2listw(soi.nb, style = \"B\", zero.policy = TRUE)\n\n\nrelative.nb &lt;- graph2nb(relativeneigh(coords))\nrelative.nb_w &lt;- nb2listw(relative.nb, style = \"W\", zero.policy = TRUE)\nrelative.nb_b &lt;- nb2listw(relative.nb, style = \"B\", zero.policy = TRUE)\n\n\ngabriel.nb &lt;- graph2nb(gabrielneigh(coords), sym=TRUE)\ngabriel.nb_w &lt;- nb2listw(gabriel.nb, style = \"W\", zero.policy = TRUE)\ngabriel.nb_b &lt;- nb2listw(gabriel.nb, style = \"B\", zero.policy = TRUE)\n\n\n## Vecinos basados en distancia\n\nknn.nb_1 &lt;- knn2nb(knearneigh(coords, k = 1))\nknn.nb_2 &lt;- knn2nb(knearneigh(coords, k = 2))\nknn.nb_3 &lt;- knn2nb(knearneigh(coords, k = 3))\nknn.nb_4 &lt;- knn2nb(knearneigh(coords, k = 4))\n\n\nknn.nb_1_w &lt;- nb2listw(knn.nb_1, style = \"W\", zero.policy = TRUE)\nknn.nb_1_b &lt;- nb2listw(knn.nb_1, style = \"B\", zero.policy = TRUE)\n\n\nknn.nb_2_w &lt;- nb2listw(knn.nb_2, style = \"W\", zero.policy = TRUE)\nknn.nb_2_b &lt;- nb2listw(knn.nb_2, style = \"B\", zero.policy = TRUE)\n\n\nknn.nb_3_w &lt;- nb2listw(knn.nb_3, style = \"W\", zero.policy = TRUE)\nknn.nb_3_b &lt;- nb2listw(knn.nb_3, style = \"B\", zero.policy = TRUE)\n\n\nknn.nb_4_w &lt;- nb2listw(knn.nb_4, style = \"W\", zero.policy = TRUE)\nknn.nb_4_b &lt;- nb2listw(knn.nb_4, style = \"B\", zero.policy = TRUE)\n\n\nLuego de haber definido las matrices de conectividad, se realiza la elección de la mejor con ayuda del test I de Morán: se realiza el test y se escoge la que de un menor \\(p\\)-valor. Primero se almacenan todas las matrices en una lista y se define una función para realizar el procedimiento.\nEl sistema de hipótesis que se quiere juzgar es el siguiente, donde \\(I\\) representa el índice de Morán (Moraga (2023)):\n\\[\n\\begin{cases}\nH_0: I= \\mathrm{E}[I] & \\text{(no hay autocorrelación espacial)}\\\\\n\\text{vs} \\\\\nH_1: I\\not=\\mathrm{E}[I] & \\text{(hay autocorrelación espacial)}\\\\\n\\end{cases}\n\\]\n\nmat &lt;- list(nepal.lw_w, nepal.lw_b,\n            tri.nb_w, tri.nb_b,\n            soi.nb_w, soi.nb_b,\n            relative.nb_w, relative.nb_b,\n            gabriel.nb_w, gabriel.nb_b,\n            knn.nb_1_w, knn.nb_1_b,\n            knn.nb_2_w, knn.nb_2_b,\n            knn.nb_3_w, knn.nb_3_b,\n            knn.nb_4_w, knn.nb_4_b)\n\n\nnames(mat) &lt;- c(paste0(\"phys_\", c(\"W\",\"B\")),\n                paste0(\"tri_\",  c(\"W\",\"B\")),\n                paste0(\"soi_\",  c(\"W\",\"B\")),\n                paste0(\"rel_\",  c(\"W\",\"B\")),\n                paste0(\"gab_\",  c(\"W\",\"B\")),\n                paste0(\"knn1_\", c(\"W\",\"B\")),\n                paste0(\"knn2_\", c(\"W\",\"B\")),\n                paste0(\"knn3_\", c(\"W\",\"B\")),\n                paste0(\"knn4_\", c(\"W\",\"B\")))\n\nmatrix_eval &lt;- function(var, weights) {\n  \n  aux &lt;- numeric(length(weights)) \n  \n  for (i in seq_along(weights)) {\n    aux[i] &lt;- moran.mc(var,\n                       weights[[i]], nsim = 9999,\n                       alternative = \"two.sided\")$p.value\n  }\n  \n  best_idx &lt;- which.min(aux)\n  \n  list(\n    best_index  = best_idx,\n    best_weight = weights[[best_idx]],\n    best_name = names(weights)[which.min(aux)],\n    best_moran  = moran.mc(var, weights[[best_idx]], nsim = 9999,\n                           alternative = \"two.sided\"),\n    all_pvalues = aux\n  )\n}\n\nSe ve cuál fue la matriz que mejor describe la correlación espacial de los datos.\n\n\nCode\nbest.w &lt;- matrix_eval(nepal.utm$ad_illit, mat)\nbest.weights &lt;- best.w$best_weight\n\nbest.weights\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 75 \nNumber of nonzero links: 366 \nPercentage nonzero weights: 6.506667 \nAverage number of links: 4.88 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 75 5625 75 32.85108 309.2524\n\n\nComo se puede ver, esta corresponde a la obtenida considerando los cuatro vecinos más cercanos con el estilo W, es decir, con las filas estandarizadas. El mapa con esta matriz de conectividad se puede ver en la Figure 2.\n\nnb_lines &lt;- nb2lines(best.weights$neighbours, coords = coords, as_sf = TRUE)\n\nst_crs(nb_lines) &lt;- st_crs(nepal.utm)\n\nggplot() +\n  geom_sf(data = nepal.utm, fill = \"grey95\", color = \"grey80\", size = 0.2) +\n  # Conexiones\n  geom_sf(data = nb_lines, color = \"#2c3e50\", size = 0.3, alpha = 0.6) +\n  \n  # Centroides\n  geom_point(data = coords, aes(x = X, y = Y), \n             color = \"#e74c3c\", size = 1, alpha = 0.8) +\n  \n  # Estética\n  theme_void() +\n  labs(title = \"Estructura de Conectividad Espacial\",\n       subtitle = paste0(\"Matriz de Pesos: \", best.w$best_name))\n\n\n\n\n\n\n\nFigure 2: Mapa de Nepal con la Mejor Estructura de Conectividad Espacial\n\n\n\n\n\nSe realizó el Test I de Moran mediante Monte Carlo con \\(n=9999\\) permutaciones. La distribución empírica del índice de Moran puede observarse en la Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Histograma de los valores simulados del índice de Moran. La línea cortada representa del valor del índice de Moran observado.\n\n\n\n\n\nDe esta forma, se evidencia que al 5% de significancia se rechaza \\(H_0: I = \\mathrm{E}[I]\\). Es decir, hay suficiente evidencia para concluir que hay presencia de correlación espacial con respecto a la tasa de adultos analfabetas en Nepal.\nEn la Figure 4 se muestra el diagrama de Moran de la tasa de adultos analfabetas en Nepal.\n\n\n\n\n\n\n\n\nFigure 4: Dispersograma de Moran para la tasa de adultos analfabetas en Nepal\n\n\n\n\n\n\nÍndice de Moran (\\(I = 0.595\\)):\nEl valor del estadístico es positivo y sustancialmente mayor que cero, lo que indica la presencia de una autocorrelación espacial global positiva fuerte*. Esto sugiere que los distritos geográficamente cercanos tienden a presentar tasas de analfabetismo similares: áreas con niveles altos tienden a agruparse espacialmente, al igual que áreas con niveles bajos.\nDado que el \\(p\\)-valor es prácticamente cero (\\(p &lt; 0.001\\)), se rechaza la hipótesis nula de ausencia de autocorrelación espacial. En consecuencia, el patrón observado no es producto del azar, sino que refleja una estructura espacial estadísticamente significativa.\n\nCuadrantes\n\n\nEl diagrama se divide en cuatro cuadrantes definidos por las medias de la tasa de analfabetismo (eje horizontal) y su rezago espacial (eje vertical). Cada cuadrante representa un tipo particular de asociación espacial:\n\n\n\n\n\n\n\n\n\n\nCuadrante\nPatrón\nDescripción\nEjemplos en la Figura\nImplicación Geográfica\n\n\n\n\nArriba–Derecha\nAlto–Alto (H–H)\nDistritos con tasas de analfabetismo altas rodeados por vecinos con valores igualmente altos.\nCasos en el extremo superior derecho.\nHotspots de analfabetismo.\n\n\nAbajo–Izquierda\nBajo–Bajo (L–L)\nDistritos con tasas de analfabetismo bajas rodeados por vecinos con valores bajos.\nLalitpur y puntos cercanos al origen.\nColdspots de analfabetismo.\n\n\nArriba–Izquierda\nBajo–Alto (L–H)\nDistritos con tasas bajas rodeados por vecinos con tasas altas.\nSurkhet.\nOutliers espaciales (rompen el patrón regional).\n\n\nAbajo–Derecha\nAlto–Bajo (H–L)\nDistritos con tasas altas rodeados por vecinos con tasas bajas.\nKapilbastu.\nOutliers espaciales con comportamiento diferenciado.\n\n\n\nLa pendiente positiva de la recta de regresión —equivalente al valor del Índice de Moran— y la concentración de observaciones en los cuadrantes Alto–Alto y Bajo–Bajo confirman que la tasa de analfabetismo adulto es un fenómeno espacialmente dependiente en Nepal. Este resultado sugiere que factores subyacentes al analfabetismo (por ejemplo, inversión en educación, pobreza estructural o condiciones socioculturales) no se distribuyen de manera aleatoria, sino que operan a escala regional y trascienden los límites administrativos distritales. En consecuencia, el diseño de políticas públicas orientadas a la reducción del analfabetismo debería adoptar un enfoque regional o interdistrital, concentrando esfuerzos en los hotspots identificados y analizando los outliers espaciales para comprender mecanismos locales y replicar experiencias exitosas."
  },
  {
    "objectID": "arealdata/datos_area.html#prueba-de-autocorrelación-local",
    "href": "arealdata/datos_area.html#prueba-de-autocorrelación-local",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Prueba de Autocorrelación Local",
    "text": "Prueba de Autocorrelación Local\nCon el fin de identificar clusters y evaluar la similitud entre áreas y sus vecinos de forma local, se realiza el test I de Moran de forma locaL. En la Figure 5 se muestran dos mapas en los que se observa autocorrelación espacial significativa en los distritos de las zonas noroccidental, central y suroriental de Nepal. En estas zonas predominan relaciones tipo alto–alto y bajo–bajo, lo que indica que los distritos con altas tasas de analfabetismo en adultos tienden a estar rodeados por distritos con tasas igualmente altas, y de igual forma para las tasas bajas.\n\nlmoran &lt;- localmoran(nepal.utm$ad_illit, \n                     best.weights, \n                     alternative = \"two.sided\")\nnepal.utm$lmp &lt;- lmoran[, \"Pr(z != E(Ii))\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Análisis de Autocorrelación Espacial Local para la tasa de analfabetismo."
  },
  {
    "objectID": "arealdata/datos_area.html#modelo-de-regresión-mco",
    "href": "arealdata/datos_area.html#modelo-de-regresión-mco",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Modelo de Regresión MCO",
    "text": "Modelo de Regresión MCO\nPrimero, se muestra el modelo de regresión que tuvo un mejor ajuste con respecto al \\(R^2\\) (0.4917) y la significancia de los coeficientes. Este modelo tiene la forma \\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\]\n\n\nCode\nform &lt;- as.formula(\"ad_illit ~ pcinc + boyg1_5 + lif40 + soc + tot\")\n\nfit.OLS &lt;- lm(form,\n              data = data)\n\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n57.9031\n11.9913\n4.8288\n0.0000\n\n\npcinc\n-0.0231\n0.0043\n-5.4282\n0.0000\n\n\nboyg1_5\n0.0002\n0.0001\n3.5416\n0.0007\n\n\nlif40\n1.5015\n0.5215\n2.8790\n0.0053\n\n\nsoc\n-25.6299\n12.2164\n-2.0980\n0.0396\n\n\ntot\n0.0096\n0.0049\n1.9531\n0.0549\n\n\n\n\n\nAdemás, se realiza el Test I de Moran con los residuales del modelo, rechazando la ausencia de autocorrelación espacial al 5% de significancia. Es decir, los residuales presentan una dependencia espacial que no se pudo explicar con el modelo.\n\nlmmorantest &lt;- lm.morantest(fit.OLS, listw = best.weights)\nt.lmmorantest &lt;- tidy(lmmorantest)\n\nkable(t.lmmorantest[,c(\"statistic\", \"p.value\", \"method\", \"alternative\")])\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n\n6.216516\n0\nGlobal Moran I for regression residuals\ngreater\n\n\n\n\n\nDe esta forma, se hace necesario ajustar modelos de regresión espacial."
  },
  {
    "objectID": "arealdata/datos_area.html#sar---spatial-autoregressive-model-spatial-lag",
    "href": "arealdata/datos_area.html#sar---spatial-autoregressive-model-spatial-lag",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "SAR - Spatial Autoregressive Model (Spatial Lag)",
    "text": "SAR - Spatial Autoregressive Model (Spatial Lag)\nEl modelo SAR incorpora la interdependencia espacial de la variable espacial con el \\(\\rho \\in [-1,1]\\). Este modelo tiene la estructura: \\[\n\\mathbf{y} = \\rho \\mathbf{W} \\mathbf{y} +  \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}, \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\]\n\n\nCode\nfit.SAR &lt;- lagsarlm(form,\n                    data = data, listw = best.weights)"
  },
  {
    "objectID": "arealdata/datos_area.html#sem---spatial-error-model",
    "href": "arealdata/datos_area.html#sem---spatial-error-model",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "SEM - Spatial Error Model",
    "text": "SEM - Spatial Error Model\nPara el modelo SEM se tiene la estructura \\[\n\\begin{aligned}\n\\mathbf{y} =& \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{u},\n\\\\\n\\mathbf{u} =& \\lambda \\mathbf{W} \\mathbf{u} + \\boldsymbol{\\varepsilon}, \\qquad\n\\boldsymbol{\\varepsilon}\n\\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\end{aligned}\n\\] y este asume que hay factores no observados que generan la dependencia espacial. Es decir, la dependencia se incorpora en los errores.\n\n\nCode\nfit.SEM &lt;- errorsarlm(form,\n                      data = data, listw = best.weights)"
  },
  {
    "objectID": "arealdata/datos_area.html#sdm---spatial-durbin-model",
    "href": "arealdata/datos_area.html#sdm---spatial-durbin-model",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "SDM - Spatial Durbin Model",
    "text": "SDM - Spatial Durbin Model\nEl modelo de Durbin integra tanto interdependencia espacial de la respuesta como efectos espaciales de las covariables. \\[\n\\mathbf{y} = \\rho \\mathbf{W} \\mathbf{y} +  \\mathbf{X} \\boldsymbol{\\beta} + \\gamma \\mathbf{W} \\mathbf{X}  + \\boldsymbol{\\varepsilon} , \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\]\n\n\nCode\nfit.Durbin &lt;- lagsarlm(form,\n                       data = data, listw = best.weights, type = \"mixed\")"
  },
  {
    "objectID": "arealdata/datos_area.html#slx---spatially-lagged-x-model",
    "href": "arealdata/datos_area.html#slx---spatially-lagged-x-model",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "SLX - Spatially Lagged X Model",
    "text": "SLX - Spatially Lagged X Model\nEn el modelo SLX se tienen en cuenta las relaciones entre las covariables de los vecinos. \\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\gamma \\mathbf{W} \\mathbf{X}  + \\boldsymbol{\\varepsilon} , \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\]\n\n\nCode\nfit.D_lagX &lt;- lmSLX(form,\n                    data = data, listw = best.weights)"
  },
  {
    "objectID": "arealdata/datos_area.html#sdem---spatial-durbin-error-model",
    "href": "arealdata/datos_area.html#sdem---spatial-durbin-error-model",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "SDEM - Spatial Durbin Error Model",
    "text": "SDEM - Spatial Durbin Error Model\nEl modelo SDEM combina los modelos SEM y SLX. Este tiene la siguiente estructura \\[\n\\begin{aligned}\n\\mathbf{y} =& \\mathbf{X} \\boldsymbol{\\beta} + \\lambda \\mathbf{W} \\mathbf{X}  + \\mathbf{u}\\\\\n\\mathbf{u} =& \\gamma\\mathbf{W} \\mathbf{u} + \\boldsymbol{\\varepsilon}, \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\end{aligned}\n\\]\n\n\nCode\nfit.SDEM &lt;- errorsarlm(form,\n                       data = data, listw = best.weights, etype = \"emixed\")"
  },
  {
    "objectID": "arealdata/datos_area.html#manski-model",
    "href": "arealdata/datos_area.html#manski-model",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Manski Model",
    "text": "Manski Model\nEs un modelo complejo de la forma \\[\n\\begin{aligned}\n\\mathbf{y} =& \\rho \\mathbf{W} \\mathbf{y} +  \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{W} \\mathbf{X} \\theta  + \\mathbf{u}, \\\\\n\\mathbf{u} =& \\lambda \\mathbf{W} \\mathbf{u} + \\boldsymbol{\\varepsilon},\n\\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\end{aligned}\n\\]\n\n\nCode\nfit.Manski &lt;- sacsarlm(form,\n                       data = data, listw = best.weights, type=\"sacmixed\")"
  },
  {
    "objectID": "arealdata/datos_area.html#sac-sarar--combined-spatial-autocorrelation-model",
    "href": "arealdata/datos_area.html#sac-sarar--combined-spatial-autocorrelation-model",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "SAC (SARAR)- Combined Spatial Autocorrelation Model",
    "text": "SAC (SARAR)- Combined Spatial Autocorrelation Model\nPor último, se tiene una combinación de modelos SAR y SEM. Es decir, se asume interdependencia espacial de la variable dependiente y factores no observados. \\[\n\\begin{aligned}\n\\mathbf{y} =& \\rho \\mathbf{W} \\mathbf{y} +  \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{u} \\\\ \\mathbf{u} =& \\lambda \\mathbf{W} \\mathbf{u}+ \\boldsymbol{\\varepsilon}, \\qquad \\boldsymbol{\\varepsilon} \\sim \\mathrm{MVN}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\n\\end{aligned}\n\\]\n\n\nCode\nfit.SARAR &lt;- sacsarlm(form,\n                      data = data, listw = best.weights, type=\"sac\")"
  },
  {
    "objectID": "arealdata/datos_area.html#selección-del-modelo",
    "href": "arealdata/datos_area.html#selección-del-modelo",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Selección del Modelo",
    "text": "Selección del Modelo\nPara realizar la selección del mejor modelo, se recurre a comparar los AIC.\n\n\n\n\nTable 1: Comparación del AIC de los modelos ajustados\n\n\n\n\n\n\nModelo\ngl\nAIC\n\n\n\n\nfit.SARAR\n9\n499.7997\n\n\nfit.SDEM\n13\n501.6684\n\n\nfit.Durbin\n13\n501.7228\n\n\nfit.SEM\n8\n503.7218\n\n\nfit.SAR\n8\n504.1058\n\n\nfit.D_lagX\n12\n531.7039\n\n\nfit.OLS\n7\n539.9456\n\n\n\n\n\n\n\n\nEn la Table 1 se puede observar que el modelo con un menor AIC corresponde al SARAR."
  },
  {
    "objectID": "arealdata/datos_area.html#interpretación",
    "href": "arealdata/datos_area.html#interpretación",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Interpretación",
    "text": "Interpretación\nDebido a que el modelo SAC/SARAR fue el de menor AIC, se muestra su resúmen. Primero, se puede ver que no todas las variables son significativas, a diferencia del modelo de regresión ajustado por mínimos cuadrados ordinarios.\n\ncoef(summary(fit.SARAR)) %&gt;% \n  kable(format = \"html\", digits = 4, align = 'c', escape = FALSE)\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n32.4605\n11.3319\n2.8645\n0.0042\n\n\npcinc\n-0.0183\n0.0032\n-5.6597\n0.0000\n\n\nboyg1_5\n0.0001\n0.0000\n1.6876\n0.0915\n\n\nlif40\n0.9806\n0.4439\n2.2090\n0.0272\n\n\nsoc\n-10.1646\n8.2507\n-1.2320\n0.2180\n\n\ntot\n0.0059\n0.0036\n1.6328\n0.1025\n\n\n\n\n\nPor otro lado, se presentan las estimaciones los parámetros \\(\\rho\\) y \\(\\lambda\\).\n\nparams.sac &lt;- data.frame(Parámetro = c(\"$\\\\rho$\", \"$\\\\lambda$\"),\n                         Estimación = c(fit.SARAR$rho, fit.SARAR$lambda),\n                         SE = c(fit.SARAR$rho.se, fit.SARAR$lambda.se))\n\nparams.sac %&gt;% \n  kable(format = \"pandoc\", digits = 4, align = 'c', row.names = FALSE)\n\n\n\n\nParámetro\nEstimación\nSE\n\n\n\n\n\\(\\rho\\)\n0.4682\n0.1891\n\n\n\\(\\lambda\\)\n0.5683\n0.2027\n\n\n\n\n\nDe esta forma, con \\(\\hat{\\rho}=0.468\\) se confirma que hay dependencia espacial con respecto a la tasa de adultos analfabetas en Nepal. Más concretamente, un distrito puede afectar ``positivamente’’ al valor de sus vecinos, justo como se veía en la Figure 5. Por otro lado, \\(\\hat{\\lambda} = 0.568\\) indica que hay factores no incluídos en el modelo que presentan dependencia espacial. En este caso, ambas estimaciones son significativas al 5%.\nPor lo tanto, se podría ver que a medida que los ingresos per cápita aumentan en un distrito, su tasa de analfabetismo disminuye. Por otro lado, si en un distrito el porcentaje de personas que no sobreviven hasta los 40 años aumenta, se espera que la tasa de analfabetismo aumente también."
  },
  {
    "objectID": "arealdata/datos_area.html#validación",
    "href": "arealdata/datos_area.html#validación",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Validación",
    "text": "Validación\n\nCorrelación Espacial\nLa primera evaluación que se realiza corresponde a ver si los residuales del mejor modelo ajustado siguen presentando autocorrelación espacial o no. Para esto, se realiza el Test I de Moran, concluyendo que, a una significancia del 5%, no se rechaza la hipótesis de ausencia de autocorrelación espacial de los residuales.\n\ndata$residuals &lt;- resid(fit.SARAR)\n\nmoran.sac &lt;- moran.test(data$residuals, listw = best.weights, alternative = \"two.sided\")\nkable(tidy(moran.sac)[,c(\"statistic\", \"p.value\", \"method\", \"alternative\")])\n\n\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n\n0.2946557\n0.7682569\nMoran I test under randomisation\ntwo.sided\n\n\n\n\n\n\n\nHomoscedasticidad\nEl siguiente supuesto que se prueba es el de varianza homogénea. Primero, en la Figure 6 se puede ver que los residuales no tienen ningún comportamiento que indique heterogeneidad de la varianza.\n\n\n\n\n\n\n\n\nFigure 6: Residuales vs Valores Ajustados para el modelo SARAR\n\n\n\n\n\nAdemás, se usa el test de Breusch-Pagan para juzgar \\(H_0: \\text{la varianza es homogénea}\\).\n\nbp.sac &lt;- bptest.Sarlm(fit.SARAR)\nkable(tidy(bp.sac)[,c(\"statistic\", \"p.value\", \"method\")])\n\n\n\n\nstatistic\np.value\nmethod\n\n\n\n\n3.93796\n0.5583824\nstudentized Breusch-Pagan test\n\n\n\n\n\nAsí, al 5% de significancia no se rechaza la hipótesis nula de homoscedasticidad.\n\n\nNormalidad\nEn la Figure 7 se puede ver que los residuales tienden a estar cerca de la línea teórica, indicando un ajuste bueno.\n\n\n\n\n\n\n\n\nFigure 7: QQ-plot de los residuales del modelo SARAR\n\n\n\n\n\nPara corroborar esto, se realiza el test de Anderson–Darling, concluyendo que no se rechaza la hipótesis de normalidad de los residuales al 5% de significancia.\n\nad.sarar &lt;- ad.test(data$residuals)\nkable(tidy(ad.sarar))\n\n\n\n\nstatistic\np.value\nmethod\n\n\n\n\n0.6159227\n0.105192\nAnderson-Darling normality test"
  },
  {
    "objectID": "arealdata/datos_area.html#estimación",
    "href": "arealdata/datos_area.html#estimación",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Estimación",
    "text": "Estimación\nA continuación, en la Figure 8 se presentan las estimaciones de la tasa de analfabetismo en adultos en Nepal.\n\nnepal.utm$fitted &lt;- fitted(fit.SARAR)\nnepal.utm$residuals &lt;- resid(fit.SARAR)\n\n\n\n\n\n\n\n\n\n\nFigure 8: Mapa de predicciones de la tasa de analfabetismo en Nepal por distrito"
  },
  {
    "objectID": "arealdata/datos_area.html#otros-modelos",
    "href": "arealdata/datos_area.html#otros-modelos",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Otros Modelos",
    "text": "Otros Modelos\nSe decide comparar los resultados con otros modelos que no incorporan parámetros de asociación espacial como se vió en la sección anterior."
  },
  {
    "objectID": "arealdata/datos_area.html#moran-eigenvalues",
    "href": "arealdata/datos_area.html#moran-eigenvalues",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Moran Eigenvalues",
    "text": "Moran Eigenvalues\nPara realizar el uso de los vectores propios de la matriz \\(\\mathbf{M}\\mathbf{W}\\mathbf{M}\\), con \\(\\mathbf{M} = \\mathbf{I} - \\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\), se usa la función SpatialFiltering() que toma el subconjunto de vectores propios que reducen la correlación espacial de los residuales en el modelo de regresión MCO.\n\n\nCode\nSF &lt;- SpatialFiltering(form, data = data, nb = best.weights$neighbours, \n                       style = \"W\", verbose = FALSE, ExactEV = TRUE)\n\nfit.SF &lt;- lm(ad_illit ~ pcinc + boyg1_5 + lif40 + soc + tot + fitted(SF),\n             data = data)\n\n\nPor lo tanto, también se presenta el resumen de este modelo, que además tiene un \\(R^2=0.8076\\).\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n57.9031\n7.1238\n8.1281\n0.0000\n\n\npcinc\n-0.0231\n0.0025\n-9.1371\n0.0000\n\n\nboyg1_5\n0.0002\n0.0000\n5.9614\n0.0000\n\n\nlif40\n1.5015\n0.3098\n4.8462\n0.0000\n\n\nsoc\n-25.6299\n7.2576\n-3.5315\n0.0008\n\n\ntot\n0.0096\n0.0029\n3.2876\n0.0017\n\n\nfitted(SF)vec3\n-38.4032\n4.9944\n-7.6892\n0.0000\n\n\nfitted(SF)vec11\n-21.0116\n4.9944\n-4.2070\n0.0001\n\n\nfitted(SF)vec15\n21.8552\n4.9944\n4.3759\n0.0001\n\n\nfitted(SF)vec1\n12.0327\n4.9944\n2.4092\n0.0193\n\n\nfitted(SF)vec4\n10.6444\n4.9944\n2.1313\n0.0375\n\n\nfitted(SF)vec2\n-9.9101\n4.9944\n-1.9842\n0.0521\n\n\nfitted(SF)vec10\n-10.3387\n4.9944\n-2.0701\n0.0431\n\n\nfitted(SF)vec12\n-9.5249\n4.9944\n-1.9071\n0.0616\n\n\nfitted(SF)vec21\n-13.9456\n4.9944\n-2.7922\n0.0071\n\n\nfitted(SF)vec19\n11.4086\n4.9944\n2.2843\n0.0262\n\n\nfitted(SF)vec8\n8.1828\n4.9944\n1.6384\n0.1069\n\n\nfitted(SF)vec20\n-9.6081\n4.9944\n-1.9238\n0.0595\n\n\nfitted(SF)vec9\n-6.9303\n4.9944\n-1.3876\n0.1708\n\n\n\n\n\n\nCorrelaciónHomoscedasticidadNormalidad\n\n\n\nSF.moran &lt;- lm.morantest(fit.SF, best.weights, alternative = \"two.sided\")\nt.SFmoran &lt;- tidy(SF.moran)\n\nkable(t.SFmoran[,c(\"statistic\", \"p.value\", \"method\", \"alternative\")])\n\n\n\n\n\n\n\n\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n\n-0.0167557\n0.9866315\nGlobal Moran I for regression residuals\ntwo.sided\n\n\n\n\n\n\n\n\n\nCode\nnepal.utm$residuals.SF &lt;- resid(fit.SF)\n\nSF.bptest &lt;- bptest(fit.SF)\n\nscatter.smooth(resid(fit.SF) ~ fitted(fit.SF),\n               xlab = \"Valores Ajustados\", ylab = \"Residuales\")\nlegend(\"bottomright\", \n       legend=c(paste0(\"Breusch-Pagan:\", round(SF.bptest$statistic, 4)), \n                paste0(expression(p), \"-valor: \", round(SF.bptest$p.value, 4))),\n       cex=1,\n       bg='salmon')\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nSF.normal &lt;- ad.test(resid(fit.SF))\n\nqqnorm(resid(fit.SF),\n       xlab = \"Cuantiles Teóricos\",\n       ylab = \"Cuantiles Muestrales\", \n       main = \"QQ-plot de los residuales\")\nqqline(resid(fit.SF))\n\nlegend(\"bottomright\", \n       legend=c(paste0(\"Anderson-Darling:\", round(SF.normal$statistic, 4)), \n                paste0(expression(p), \"-valor: \", round(SF.normal$p.value, 4))),\n       cex=1,\n       bg='salmon')\n\n\n\n\n\n\n\n\n\n\n\n\nDe esta forma, es posible ver que, por ejemplo, los residuales ya no presentan correlación espacial y el supuesto de normalidad se cumple. Lo único que no se cumple es la homoscedasticidad."
  },
  {
    "objectID": "arealdata/datos_area.html#gam",
    "href": "arealdata/datos_area.html#gam",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "GAM",
    "text": "GAM\nOtra opción es usar los modelos GAM. Una ventaja de ellos, es que permite modelar covariables que no tengan una relación lineal con la variable respuesta. La forma de incluir la correlación espacial es mediante las coordenadas: s(X, Y), como se muestra en Bivand, Pebesma, and Gómez-Rubio (2008).\n\n\nCode\nlibrary(mgcv)\n\ndata &lt;- cbind(data, coords)\n\nfit.gam &lt;- gam(ad_illit ~ s(pcinc, bs = \"cr\") + boyg1_5\n                          + s(lif40, bs = \"cr\") + s(soc, bs = \"cr\")\n                          + s(tot, bs = \"cr\") + s(X, Y), data = data)\n\n\nLa única forma de interpretar los suavizamientos es con los efectos parciales de las covariables sobre la variable respueta. De la misma forma, se presenta su diagnóstivco.\n\n\n\n\n\n\n\n\n\n\n\n(a) Efecto parcial de pcinc\n\n\n\n\n\n\n\n\n\n\n\n(b) Efecto parcial de lif40\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Efecto parcial de soc\n\n\n\n\n\n\n\n\n\n\n\n(d) Efecto parcial de tot\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Efecto espacial estimado\n\n\n\n\n\n\n\nFigure 9: Efectos parciales sobre la tasa de analfabetismo\n\n\n\n\nCorrelaciónHomoscedasticidadNormalidad\n\n\n\nnepal.utm$residuals.gam &lt;- resid(fit.gam)\n\ngam.moran &lt;-  moran.test(nepal.utm$residuals.gam, best.weights, alternative =  \"two.sided\")\n\ntgam.moran &lt;- tidy(gam.moran)\n\nkable(tgam.moran[,c(\"statistic\", \"p.value\", \"method\", \"alternative\")])\n\n\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n\n-2.492309\n0.0126916\nMoran I test under randomisation\ntwo.sided\n\n\n\n\n\n\n\n\n\nCode\ngam.bptest &lt;- bptest(fit.gam)\n\nscatter.smooth(resid(fit.gam) ~ fitted(fit.gam),\n               xlab = \"Valores Ajustados\", ylab = \"Residuales\")\nlegend(\"bottomright\", \n       legend=c(paste0(\"Breusch-Pagan:\", round(gam.bptest$statistic, 4)), \n                paste0(expression(p), \"-valor: \", round(gam.bptest$p.value, 4))),\n       cex=1,\n       bg='salmon')\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngam.normal &lt;- ad.test(resid(fit.gam))\n\nqqnorm(resid(fit.gam),\n       xlab = \"Cuantiles Teóricos\",\n       ylab = \"Cuantiles Muestrales\", \n       main = \"QQ-plot de los residuales\")\nqqline(resid(fit.gam))\n\nlegend(\"bottomright\", \n       legend=c(paste0(\"Anderson-Darling:\", round(gam.normal$statistic, 4)), \n                paste0(expression(p), \"-valor: \", round(gam.normal$p.value, 4))),\n       cex=1,\n       bg='salmon')\n\n\n\n\n\n\n\n\n\n\n\n\nEl supuesto de no correlación espacial de los residuales se rechaza con una significancia del 5%. Con la introducción de las coordenadas no se alcanza a elimianar la dependencia espacial de los residuales."
  },
  {
    "objectID": "arealdata/datos_area.html#comparación-estimaciones",
    "href": "arealdata/datos_area.html#comparación-estimaciones",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Comparación Estimaciones",
    "text": "Comparación Estimaciones\nSe presentan las estimaciones para los tres modelos ajustados en la Figure 10.\n\nnepal.utm$fitted.SF &lt;- fitted(fit.SF)\nnepal.utm$fitted.gam &lt;- fitted(fit.gam)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Comparación de las de predicciones de la tasa de analfabetismo en Nepal por distrito"
  },
  {
    "objectID": "arealdata/datos_area.html#comparación-predicciones",
    "href": "arealdata/datos_area.html#comparación-predicciones",
    "title": "Análisis de Datos de Área: El Analfabetismo en Nepal",
    "section": "Comparación Predicciones",
    "text": "Comparación Predicciones\nSe presentan las predicciones para los tres modelos ajustados en la Figure 10.\n\nnepal.utm$fitted.SF &lt;- fitted(fit.SF)\nnepal.utm$fitted.gam &lt;- fitted(fit.gam)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Comparación de las de predicciones de la tasa de analfabetismo en Nepal por distrito"
  }
]